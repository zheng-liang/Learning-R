---
title: "Estimating Volatility of Stock Returns Using GARCH Models"
author: "Tan Zheng Liang"
date: "2022-07-16"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    code_folding: show
    highlight: tango
    theme: flatly
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(purl = knitr::hook_purl)
```

## 1 Introduction

The purpose of this project was to learn about Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models and how to implement them in R to measure volatility of stock returns. It introduces the packages in R that is used for building GARCH models and briefly touches on basic n-ahead forecasting to evaluate the models' out-of-sample forecast performance.

## 2 Packages Required

```{r load packages, message=FALSE, warning=FALSE}
library(dplyr)
library(forecast) # For ARIMA models, forecasting and evaluation
library(PerformanceAnalytics) # For portfolio performance and risk analysis
library(PortfolioAnalytics) # For portfolio optimization and analysis
library(quantmod) # For obtaining historical prices from Yahoo Finance
library(rugarch) # For univariate GARCH models
library(urca) # For unit root tests
```

## 3 Understanding GARCH Models

In typical time series analysis, we usually require that the time series data is stationary with a constant mean and constant variance. However, we may have stationarity with changing conditional variance (conditional heteroskedasticity), commonly seen in financial data. In such data, the plots would show periods of volatility clustering, where there are periods of high and low volatility and variance is not constant in these periods. For instance, let us take a look at the weekly returns of Apple Inc. (AAPL), which we will be using for the rest of this project.

```{r load AAPL price data}
# Load AAPL price data from Yahoo Finance using quantmod package

# Set start and end dates for data retrieval
startdate <- as.Date("2010-01-01")
enddate <- as.Date("2022-07-01")

# Retrieve price data
AAPL_price <- quantmod::getSymbols(Symbols = "AAPL", 
                                   src = "yahoo", 
                                   from = startdate, to = enddate, 
                                   periodicity = "weekly", 
                                   auto.assign = F)

# View the first and last 6 observations in AAPL_price
c(head(AAPL_price), tail(AAPL_price))

# Check number of observations
nrow(AAPL_price)

# Check for missing data
colSums(is.na(AAPL_price))
```

```{r calculate log returns, fig.align='center'}
# Calculate log/continuous returns using Adjusted column (column 6)

# Remove first row of calculated returns since returns cannot be calculated for first observation
rAAPL <- PerformanceAnalytics::Return.calculate(prices = AAPL_price[, 6], method = "log")[-1,] * 100
colnames(rAAPL) <- "Returns"

# Check that first row has been removed
head(rAAPL)
nrow(rAAPL)

# Chart weekly log-returns over time
plot.zoo(rAAPL, main = "Weekly Log-Returns of AAPL", xlab = "Time", ylab = "Log-Return (in %)")
```

If we were to only look at the period in early 2020 (COVID-19 outbreak), we see that there is a period where large absolute returns are followed by large absolute returns, which then starts to return to "normal". I charted the monthly volatilty (measured using annualized standard deviation) of `rAAPL` to better present the fluctuations in returns that might not be easily seen in the weekly returns plot.

```{r rolling volatility of AAPL returns, fig.align='center'}
# Plot rolling volatility of AAPL returns

# width = 4 to approximately calculate annualized sd using a monthly rolling-window
PerformanceAnalytics::chart.RollingPerformance(R = rAAPL, width = 4, FUN = "sd.annualized", scale = 52, main = "1-Month Rolling Volatility of AAPL Returns")
```

We can check if the series is stationary using an Augmented Dickey Fuller Test and Kwiatkowski-Phillips-Schmidt-Shin Test.

```{r ADF test on AAPL returns}
# ADF Test with Drift, lags selected based on AIC

rAAPL %>% urca::ur.df(type = "drift", selectlags = "AIC") %>% summary()

# KPSS Test

rAAPL %>% urca::ur.kpss(type = "mu", lags = "long") %>% summary()
```

The test-statistic value of `tau2` -17.95 against the critical value of -2.86 at the 5% significance level means that we can reject the null hypothesis (alternative hypothesis is that the time series does not have unit root). The KPSS test-statistic 0.052 against the 5% critical value of 0.46 means that we cannot reject the null hypothesis (alternative hypothesis that the time series is not stationary).

Despite varying variances at different periods, the ADF and KPSS tests shows that the series is stationary. The problem of conditional heteroskedasticity is not addressed in ARIMA and other models that assumed constant variance. We can use GARCH models to estimate this volatility to better manage risk and improve forecasts of returns. Before discussing GARCH, it may be helpful to talk about the ARCH model since GARCH was an improvement of it.

### 3.1 ARCH Models

ARCH or AutoRegressive Conditional Heteroskedasticity models can be thought of as an Autoregressive model. Suppose that the returns was modelled as such (remains true for all the GARCH models):

$$r_t = \mu + \varepsilon_t$$

where $\mu$ can be a constant or represented with ARMA models, and $\varepsilon_t$ is the residual at time $t$. $\varepsilon_t$ can be separated into a time-varying volatility or standard deviation $\sigma_t$ and a white noise error term (or called innovations/shocks) $z_t$ which is i.i.d. with mean of 0 and variance of 1, which we write as $\varepsilon_t = \sigma_t z_t$. 

In most cases, $z_t$ is assumed to have a normal distribution, but with the fat tails and slight skew that is typical of financial returns, it is also possible to use a Student's t-distribution or a skewed t-distribution.

```{r distribution of AAPL returns, fig.align='center'}
# Plot distribution of AAPL's weekly log returns

PerformanceAnalytics::chart.Histogram(R = rAAPL,
                                      main = "Distribution of AAPL Weekly Log-Returns", 
                                      methods = c("add.density", "add.normal"), 
                                      colorset = c("blue", "red", "black"),
                                      ylim = c(0, 0.15))

legend(x = "topleft", legend = c("Log-Return", "Density", "Normal"), col = c("blue", "red", "black"), lty = 1)
```

```{r descriptive stats of AAPL returns}
# Look at skewness and kurtosis of AAPL returns

PerformanceAnalytics::table.Distributions(R = rAAPL)
```

An ARCH(1) model is then formulated as:

$$Var(\varepsilon_t) = \sigma_t^2 = \omega + \alpha_1 \varepsilon^2_{t-1}$$

What the formula essentially say is that the conditional variance in the current period ($\sigma_t^2$) is affected by the errors in the previous period. $\alpha_0 > 0$ since variance of first period should also be non-negative and $\alpha_1 > 0$ so that larger error terms in the previous period means higher conditional variance but $\alpha_1 < 1$ to prevent "explosive" variance. This can be extended to an ARCH(q) model where we have $q$ lags of error terms.

Moving forward, I would estimate the models using returns data up to end-April 2022 and use the data in May and June 2022 to evaluate the models.

```{r split data}
# Split the data for training and testing models
trainset <- rAAPL["/2022-04",]
nrow(trainset)

testset <- rAAPL["2022-05/",]
nrow(testset)
```

I used an ARMA model to estimate the mean equation before generating the ARCH model.

```{r estimate mean equation using ARMA model}
# Find mean equation using auto.arima from forecast package

meaneqn <- forecast::auto.arima(y = trainset, 
                                max.p = 2, max.q = 2, 
                                stepwise = F, approximation = F, trace = F, 
                                ic = "aic", method = "ML")
summary(meaneqn)
```

Based on the ARMA output, a ARMA(2, 3) with non-zero mean has the lowest AIC. With this, we can create the model specification for the ARCH model using **`ugarchspec`**.

```{r create ARCH(1) model specification}
# Create ARCH(1) model specification using rugarch package, arguments to be in a list
# Check ?ugarchspec for the arguments

# Specify ARCH(1) model by setting order of GARCH terms to 0
archmodel <- list(model = "sGARCH", garchOrder = c(1, 0))

# Specify mean model as found using auto.arima
meanmodel <- list(armaOrder = c(0, 0), include.mean = T)

# Assume a normal distribution of the error term z_t
archspec <- rugarch::ugarchspec(variance.model = archmodel, mean.model = meanmodel, distribution.model = "norm")
```

```{r fit ARCH(1) model with normal distribution}
# Fit the data to the ARCH(1) specification

fitARCH <- rugarch::ugarchfit(spec = archspec, data = trainset, solver = "hybrid")

fitARCH
```

The output is split into several tables. The first table `Optimal Parameters` gives us the coefficients, standard errors and significance for the mean and ARCH equations, as well as the robust errors. Using the values, we would write the equations:

$$\hat{r}_t = 0.519 + \hat{\varepsilon}_t \\ \widehat{Var(\varepsilon_t)} = \hat{\sigma}_t^2 = 12.007 + 0.108 \varepsilon_{t-1}^2$$

However, coefficient for $\varepsilon_{t-1}^2$ is insignificant at the 5% significance level. The second table `Information Criteria` shows the different information criteria values, which can be compared across models to check which fits the data better. The third table `Weighted Ljung-Box Test on Standardized Residuals` shows results for serial correlation in the residuals and based on the test, the null hypothesis could not be rejected. The next table that interests us is the `Weighted ARCH LM Tests` and the low p-values means that we reject the null hypothesis (alternative is that there is ARCH effects). We may need to increase the order of ARCH terms to account for the rest of the ARCH effects. The last table `Adjusted Pearson Goodness-of-Fit Test` shows that the assumption of normally distributed errors cannot be rejected, but it is still good practice to try other distributions.

I created an ARCH(p) model to account for remaining ARCH effects, still using the assumed normal distribution of errors.

```{r ARCH(2) model with normal distribution}
# Specify an ARCH(2) model
arch2model <- list(model = "sGARCH", garchOrder = c(2, 0))

# Assume normal distribution of error term z_t
arch2spec <- rugarch::ugarchspec(variance.model = arch2model, mean.model = meanmodel, distribution.model = "norm")

fitARCH2 <- ugarchfit(spec = arch2spec, data = trainset, solver = "hybrid")

fitARCH2
```

ARCH LM Tests show that we cannot reject the null hypothesis, so we have sufficiently accounted for ARCH effects in the model.

I tried an ARCH(2) model with a Student's t-distribution to account for the fat-tailed distribution.

```{r ARCH(2) model with t-distribution}
# Assume a normal distribution of the error term z_t
arch2spec_std <- rugarch::ugarchspec(variance.model = arch2model, mean.model = meanmodel, distribution.model = "std")

fitARCH2_std <- ugarchfit(spec = arch2spec_std, data = trainset, solver = "hybrid")

fitARCH2_std
```

With the t-distribution ARCH(2) model, the coefficient of $\varepsilon_{t-1}^2$ is still insignificant, but we have lower AIC.

Finally, I created an ARCH(2) model with a skewed t-distribution to account for the heavy tail and skewed distribution.

```{r ARCH(2) model with skewed t-distribution}
# Use skewed t-distribution

arch2spec_sstd <- ugarchspec(variance.model = arch2model, mean.model = meanmodel, distribution.model = "sstd")

fitARCH2_sstd <- rugarch::ugarchfit(spec = arch2spec_sstd, data = trainset, solver = "hybrid")

fitARCH2_sstd
```

The ARCH(2) with skewed t-distribution has slightly higher AIC than the ARCH(2) with t-distribution, but the skew coefficient is significant.

```{r plot ARCH(2) with t-distribution, fig.align='center'}
# Plot some data from the ARCH(2) with t-distribution

par(mfrow = c(2, 2))

# Plot 1: Actual return series with conditional SD
# Plot 3: Conditional volatility vs absolute returns
# Plot 8: Distribution of residuals
# Plot 9: Q-Q plot of residuals to check for normality

plotnum <- c(1, 3, 8, 9)

for (i in plotnum) {
  plot(fitARCH2_std, which = i)
}
```

### 3.2 Standard GARCH Models

In order to capture the effects of volatility clustering, an ARCH model with many lags may be required. To solve this issue, the standard Generalized ARCH (sGARCH) can be used. The model can be thought of as an Autoregressive Moving Average Model, and is the basic model of in the GARCH variation of models. A GARCH(1, 1) model can be written as:

$$\sigma_t^2 = \alpha_0 + \alpha_1 \varepsilon_{t-1}^2 + \beta_1 \sigma_{t-1}^2 ~, \text{ where } \alpha_0,~ \alpha_1,~ \beta_1 > 0$$

Essentially, the conditional variance in the current period is affected by the errors and the conditional variance in the previous period. Similar to ARCH, the model can be extended to a GARCH(p, q) model with $p$ lags of conditional variance and $q$ lags of error terms.

Create an GARCH(1, 1) model specification with normally distributed errors:

```{r GARCH(1,1) with normal distribution}
# Specify GARCH(1,1) model 
garchmodel <- list(model = "sGARCH", garchOrder = c(1, 1))

# Assume a normal distribution of the error term z_t
garchspec <- rugarch::ugarchspec(variance.model = garchmodel, mean.model = meanmodel, distribution.model = "norm")

# Fit GARCH(1, 1)
fitGARCH11 <- rugarch::ugarchfit(spec = garchspec, data = trainset, solver = "hybrid")

fitGARCH11
```

With a GARCH(1, 1) model, the $\alpha_1$ is statistically significant, standardized residuals do not have serial correlation and there are no ARCH effects left after including the GARCH term. The Adjusted Pearson Goodness-of-Fit Test statistic also shows that we cannot reject that the errors follow a normal distribution. We can write the formula as:

$$\hat{r}_t = 0.605 + \hat{\sigma}_t \hat{z}_t \\ \widehat{Var(\varepsilon_t)} = \hat{\sigma}_t^2 = 3.732 + 0.109 \varepsilon_{t-1}^2 + 0.611 \sigma_{t-1}^2$$

As per the ARCH models, I created GARCH(1, 1) with the assumption that errors follow a t-distribution and a skewed t-distribution.

```{r GARCH(1,1) with t-distribution}
# GARCH(1, 1) with errors that follow a t-distribution

garchspec_std <- rugarch::ugarchspec(variance.model = garchmodel, mean.model = meanmodel, distribution.model = "std")

fitGARCH11_std <- rugarch::ugarchfit(spec = garchspec_std, data = trainset, solver = "hybrid")

fitGARCH11_std
```

Using the t-distribution led to insignificant $\omega$ and $\alpha_1$.

```{r GARCH(1,1) with skewed t-distribution}
# GARCH(1, 1) with errors that follow a skewed t-distribution

garchspec_sstd <- rugarch::ugarchspec(variance.model = garchmodel, mean.model = meanmodel, distribution.model = "sstd")

fitGARCH11_sstd <- rugarch::ugarchfit(spec = garchspec_sstd, data = trainset, solver = "hybrid")

fitGARCH11_sstd
```

```{r plot GARCH(1,1) with t-distribution, fig.align='center'}
# Plot some data from the GARCH(1, 1) with t-distribution

par(mfrow = c(2, 2))

for (i in plotnum) {
  plot(fitGARCH11_std, which = i)
}
```

### 3.3 Glosten-Jagannathan-Runkle GARCH Models

GJR-GARCH models can capture the asymmetric patterns in volatility due to positive and negative shocks (such as bad/good financial results, etc.). The volatility asymmetry is not captured by the standard GARCH models. We write the GJR-GARCH(1, 1) as:

$$\sigma_t^2 = \omega + \alpha_1 \varepsilon_{t-1}^2 + \gamma_1 \varepsilon_{t-1}^2 I_{t-1} + \beta_1 \sigma_{t-1}^2$$

$I_{t-1} = 0$ if $\varepsilon_{t-1}^2 \ge 0$ and $I_{t-1} = 1$ if $\varepsilon_{t-1}^2 < 0$. The inclusion of the dummy variable $I_{t-1}$ simply means that we expect higher volatility in the current period, if there was a negative shock in the previous period.

Create a GJR-GARCH(1, 1) model specification with normally distributed errors:

```{r GJR-GARCH(1,1) with normal distribution}
# Specify GJR-GARCH(1,1) model 
gjrmodel <- list(model = "gjrGARCH", garchOrder = c(1, 1))

# Assume a normal distribution of the error term z_t
gjrspec <- rugarch::ugarchspec(variance.model = gjrmodel, mean.model = meanmodel, distribution.model = "norm")

# Fit GARCH(1, 1)
fitGJR11 <- rugarch::ugarchfit(spec = gjrspec, data = trainset, solver = "hybrid")

fitGJR11
```

With the GJR-GARCH(1, 1) with normally distributed errors, none of the coefficients in the models are significant. This might hint at a mis-specification of models. Let's try the assumption of t-distribution and skewed t-distribution.

```{r GJR-GARCH(1,1) with t-distribution}
# Assume Student's t-distribution of error term z_t
gjrspec_std <- rugarch::ugarchspec(variance.model = gjrmodel, mean.model = meanmodel, distribution.model = "std")

# Fit GARCH(1, 1)
fitGJR11_std <- rugarch::ugarchfit(spec = gjrspec_std, data = trainset, solver = "hybrid")

fitGJR11_std
```

```{r GJR-GARCH(1,1) with skewed t-distribution}
# Assume skewed t-distribution of error term z_t
gjrspec_sstd <- rugarch::ugarchspec(variance.model = gjrmodel, mean.model = meanmodel, distribution.model = "sstd")

# Fit GARCH(1, 1)
fitGJR11_sstd <- rugarch::ugarchfit(spec = gjrspec_sstd, data = trainset, solver = "hybrid")

fitGJR11_sstd
```

With the t-distribution and skewed t-distribution assumption, only the coefficient of $\varepsilon_{t-1}^2$ is insignificant and $\gamma_1$ is insignificant using robust errors.

```{r plot GJR-GARCH(1,1) with t-distribution, fig.align='center'}
# Plot some data from the GJR-GARCH(1, 1) with t-distribution

par(mfrow = c(2, 2))

for (i in plotnum) {
  plot(fitGJR11_std, which = i)
}
```

### 3.4 Exponential GARCH Models

EGARCH models, similar to GJR-GARCH models, resolves the asymmetric patterns of volatility to positive and negative shocks. An EGARCH(1, 1) model is written as:

$$\ln(\sigma_t^2) = \omega + \bigg[ \alpha_1 z_{t-1} + \gamma_1 \bigg( |z_{t-1}| - E|z_{t-1}| \bigg) \bigg] + \beta_1 \ln(\sigma_{t-1}^2)$$

$\gamma_1$ is the coefficient of the EGARCH term $\gamma_1 \bigg( |z_{t-1}| - E|z_{t-1}| \bigg)$. If negative shocks cause higher volatility than positive shocks, then $\alpha_1$ should be negative. This is because, if $z_{t-1} \ge 0$, we would have $(-\alpha_1 + \gamma_1)z_{t-1} - \gamma_1 E(|z_{t-1}|)$ and if $z_{t-1} < 0$, we would have $(\alpha_1 + \gamma_1)z_{t-1} - \gamma_1 E(|z_{t-1}|)$, which creates higher conditional volatility when there is a negative shock.

Create an EGARCH(1, 1) model specification with normally distributed errors:

```{r EGARCH(1,1) with normal distribution}
# Specify EGARCH(1,1) model 
egarchmodel <- list(model = "eGARCH", garchOrder = c(1, 1))

# Assume a normal distribution of the error term z_t
egarchspec <- rugarch::ugarchspec(variance.model = egarchmodel, mean.model = meanmodel, distribution.model = "norm")

# Fit GARCH(1, 1)
fitEGARCH11 <- rugarch::ugarchfit(spec = egarchspec, data = trainset, solver = "hybrid")

fitEGARCH11
```

Based on the output, we can write the formula as:

$$\hat{r}_t = 0.535 + \hat{\sigma}_t \hat{z}_t \\ \ln(\hat{\sigma}_t^2) = 1.045 - 0.275z_{t-1} + 0.163 \bigg( |z_{t-1}| - E|z_{t-1}| \bigg) + 0.587 \ln(\sigma_{t-1}^2)$$

All the coefficients are significant, standardized residuals are not serially correlated and ARCH effects have been sufficiently captured. I have generated the EGARCH with assumed t-distribution and skewed t-distribution of errors.

```{r EGARCH(1,1) with t-distribution}
# Assume a t-distribution of the error term z_t
egarchspec_std <- rugarch::ugarchspec(variance.model = egarchmodel, mean.model = meanmodel, distribution.model = "std")

# Fit GARCH(1, 1)
fitEGARCH11_std <- rugarch::ugarchfit(spec = egarchspec_std, data = trainset, solver = "hybrid")

fitEGARCH11_std
```

```{r EGARCH(1,1) with skewed t-distribution}
# Assume a skewed t-distribution of the error term z_t
egarchspec_sstd <- rugarch::ugarchspec(variance.model = egarchmodel, mean.model = meanmodel, distribution.model = "sstd")

# Fit GARCH(1, 1)
fitEGARCH11_sstd <- rugarch::ugarchfit(spec = egarchspec_sstd, data = trainset, solver = "hybrid")

fitEGARCH11_sstd
```

```{r plot EGARCH(1,1) with t-distribution, fig.align='center'}
# Plot some data from the EGARCH(1, 1) with t-distribution

par(mfrow = c(2, 2))

for (i in plotnum) {
  plot(fitEGARCH11_std, which = i)
}
```

### 3.5 Summary

In this section, I have estimated 4 different models, with 3 different assumed distribution of the error $z_t$. I have created a table comparing their AIC values below.

| Models          | Normal Distribution | t-Distribution | Skewed t-Distribution |
|:----------------|:-------------------:|:--------------:|:---------------------:|
| ARCH(2)         | 5.397               | 5.366          | 5.367                 |
| GARCH(1, 1)     | 5.409               | 5.367          | 5.366                 |
| GJR-GARCH(1, 1) | 5.373               | 5.343          | 5.341                 |
| EGARCH(1, 1)    | 5.371               | 5.344          | 5.343                 |

Despite the rather low AIC values of the GJR models, the $\gamma$ coefficient for the $\varepsilon_{t-1}^2 I_{t-1}$ term was insignificant when looking at the robust errors. However, I would still consider this model in the forecast evaluation in the next section.

## 4 Evaluating Forecast Performance

In this section, I evaluated the out-of-sample forecast performance of the 12 models created in Section 3. However, before jumping into the forecast, there is a need to highlight the difficulty of evaluating GARCH models. The difficulty lies in using an appropriate proxy of volatility, since it is not observable at a point in time. The paper by Wennström (2014) describes some proxies of volatility, such as squared returns, using moving averages of variance and cumulative squared intra-day returns. In the paper, and another by Schmidt (2021), the High-Low Range proxy was used and the formula is given by:

$$v_t = \ln \bigg( \frac{H_t}{L_t} \bigg)$$

I extracted the weekly high and weekly low prices of AAPL of the test period and calculate the volatility proxy.

```{r high low price}
# Extract high (column 2) and low (column 3) prices within test period

proxy <- AAPL_price["2022-05/", 2:3]

proxy$Volatility <- log(proxy[, 1] / proxy[, 2]) * 100

proxy
```

In the following sub-sections, I evaluated the models against the volatility proxy using Mean Squared Error, Root Mean Squared Error and Mean Absolute Error, grouped by the model types (ARCH, GARCH, GJR and EGARCH). 

The formula for each evaluation metric is:

$$
MSE = \frac{1}{n} \sum_{n=1}^N (v_{n+1}^2 - \hat{\sigma}_{n+1}^2)^2 \\
RMSE = \sqrt{MSE} \\
MAE = \frac{1}{n} \sum_{n=1}^N |v_{n+1}^2 - \hat{\sigma}_{n+1}^2|
$$

### 4.1 ARCH Forecasts

```{r forecast ARCH(2) models}
# Forecast ARCH(2) with normal distribution of innovations 8 periods ahead
fcstARCH2_nd <- rugarch::ugarchforecast(fitORspec = fitARCH2, n.ahead = 8)

# Forecast ARCH(2) with t-distribution of innovations 8 periods ahead
fcstARCH2_std <- rugarch::ugarchforecast(fitORspec = fitARCH2_std, n.ahead = 8)

# Forecast ARCH(2) with t-distribution of innovations 8 periods ahead
fcstARCH2_sstd <- rugarch::ugarchforecast(fitORspec = fitARCH2_sstd, n.ahead = 8)
```

Store forecasted conditional standard deviation of ARCH models into a data frame:

```{r store forecast of ARCH}
fcstARCH2 <- cbind(sigma(fcstARCH2_nd), sigma(fcstARCH2_std), sigma(fcstARCH2_sstd)) %>%
  `colnames<-`(c("ARCH2_ND", "ARCH2_STD", "ARCH2_SSTD"))

fcstARCH2
```

Calculate evaluation metrics for each ARCH model:

```{r evaluation metrics for ARCH(2) models}
evalARCH <- NULL

for (i in 1:3) {
  mse <- mean((proxy$Volatility^2 - fcstARCH2[,i]^2)^2)
  
  rmse <- sqrt(mse)
  
  mae <- mean(abs(proxy$Volatility^2 - fcstARCH2[,i]^2))
  
  evalARCH <- rbind(evalARCH, c(MSE = mse, RMSE = rmse, MAE= mae))
}

rownames(evalARCH) <- colnames(fcstARCH2)

evalARCH
```

### 4.2 GARCH Forecasts

```{r forecast GARCH(1,1) models}
# Forecast GARCH(1, 1) with normal distribution of innovations 8 periods ahead
fcstGARCH11_nd <- rugarch::ugarchforecast(fitORspec = fitGARCH11, n.ahead = 8)

# Forecast GARCH(1, 1) with t-distribution of innovations 8 periods ahead
fcstGARCH11_std <- rugarch::ugarchforecast(fitORspec = fitGARCH11_std, n.ahead = 8)

# Forecast GARCH(1, 1) with t-distribution of innovations 8 periods ahead
fcstGARCH11_sstd <- rugarch::ugarchforecast(fitORspec = fitGARCH11_sstd, n.ahead = 8)
```

Store forecasted conditional standard deviation of GARCH models into a data frame:

```{r store forecast of GARCH}
fcstGARCH11 <- cbind(sigma(fcstGARCH11_nd), sigma(fcstGARCH11_std), sigma(fcstGARCH11_sstd)) %>%
  `colnames<-`(c("GARCH11_ND", "GARCH11_STD", "GARCH11_SSTD"))

fcstGARCH11
```

Calculate evaluation metrics for each GARCH model:

```{r evaluation metrics for GARCH(1,1) models}
evalGARCH <- NULL

for (i in 1:3) {
  mse <- mean((proxy$Volatility^2 - fcstGARCH11[,i]^2)^2)
  
  rmse <- sqrt(mse)
  
  mae <- mean(abs(proxy$Volatility^2 - fcstGARCH11[,i]^2))
  
  evalGARCH <- rbind(evalGARCH, c(MSE = mse, RMSE = rmse, MAE= mae))
}

rownames(evalGARCH) <- colnames(fcstGARCH11)

evalGARCH
```

### 4.3 GJR-GARCH Forecasts

```{r forecast GJR-GARCH(1,1) models}
# Forecast GJR-GARCH(1, 1) with normal distribution of innovations 8 periods ahead
fcstGJR11_nd <- rugarch::ugarchforecast(fitORspec = fitGJR11, n.ahead = 8)

# Forecast GJR-GARCH(1, 1) with t-distribution of innovations 8 periods ahead
fcstGJR11_std <- rugarch::ugarchforecast(fitORspec = fitGJR11_std, n.ahead = 8)

# Forecast GJR-GARCH(1, 1) with t-distribution of innovations 8 periods ahead
fcstGJR11_sstd <- rugarch::ugarchforecast(fitORspec = fitGJR11_sstd, n.ahead = 8)
```

Store forecasted conditional standard deviation of GJR-GARCH models into a data frame:

```{r store forecast of GJR-GARCH}
fcstGJR11 <- cbind(sigma(fcstGJR11_nd), sigma(fcstGJR11_std), sigma(fcstGJR11_sstd)) %>%
  `colnames<-`(c("GJR11_ND", "GJR11_STD", "GJR11_SSTD"))

fcstGJR11
```

Calculate evaluation metrics for each GJR-GARCH model:

```{r evaluation metrics for GJR-GARCH(1,1) models}
evalGJR <- NULL

for (i in 1:3) {
  mse <- mean((proxy$Volatility^2 - fcstGJR11[,i]^2)^2)
  
  rmse <- sqrt(mse)
  
  mae <- mean(abs(proxy$Volatility^2 - fcstGJR11[,i]^2))
  
  evalGJR <- rbind(evalGJR, c(MSE = mse, RMSE = rmse, MAE= mae))
}

rownames(evalGJR) <- colnames(fcstGJR11)

evalGJR
```

### 4.4 EGARCH Forecasts

```{r forecast EGARCH(1,1) models}
# Forecast EGARCH(1, 1) with normal distribution of innovations 8 periods ahead
fcstEGARCH11_nd <- rugarch::ugarchforecast(fitORspec = fitEGARCH11, n.ahead = 8)

# Forecast EGARCH(1, 1) with t-distribution of innovations 8 periods ahead
fcstEGARCH11_std <- rugarch::ugarchforecast(fitORspec = fitEGARCH11_std, n.ahead = 8)

# Forecast EGARCH(1, 1) with t-distribution of innovations 8 periods ahead
fcstEGARCH11_sstd <- rugarch::ugarchforecast(fitORspec = fitEGARCH11_sstd, n.ahead = 8)
```

Store forecasted conditional standard deviation of EGARCH models into a data frame:

```{r store forecast of EGARCH}
fcstEGARCH11 <- cbind(sigma(fcstEGARCH11_nd), sigma(fcstEGARCH11_std), sigma(fcstEGARCH11_sstd)) %>%
  `colnames<-`(c("EGARCH11_ND", "EGARCH11_STD", "EGARCH11_SSTD"))

fcstEGARCH11
```

Calculate evaluation metrics for each EGARCH model:

```{r evaluation metrics for EGARCH(1,1) models}
evalEGARCH <- NULL

for (i in 1:3) {
  mse <- mean((proxy$Volatility^2 - fcstEGARCH11[,i]^2)^2)
  
  rmse <- sqrt(mse)
  
  mae <- mean(abs(proxy$Volatility^2 - fcstEGARCH11[,i]^2))
  
  evalEGARCH <- rbind(evalEGARCH, c(MSE = mse, RMSE = rmse, MAE= mae))
}

rownames(evalEGARCH) <- colnames(fcstEGARCH11)

evalEGARCH
```

### 4.5 Summary

Summarizing the evaluation metrics of all the models:

```{r summary of evaluation metrics}
rbind(evalARCH, evalGARCH, evalGJR, evalEGARCH)
```

The EGARCH(1, 1) model with assumed normal distribution of innovations had the best MSE and RMSE, but it was surprising that the GJR-GARCH(1, 1) with assumed t-distribution of innovations had the best MAE. The EGARCH and GJR-GARCH models seem to be able to capture the higher volatility in the first 2 forecast periods better than the standard ARCH and GARCH models, likely due to the negative returns in those periods. However, all models were weak in capturing the volatility of returns past the 4th period in the forecast period using the basic forecast function. Since forecasting many periods ahead is bound to fail regardless of the type of models used, it is better to keep the forecast period short when using such models.

## 5 Final Remarks

While the project intended to show how ARCH and GARCH (and its variants) can be used in R to model volatility, I believe there is much to be desired from the forecasting results in Section 4. Due to the innovations ($z_t$) that cannot be observed, the forecasts does not include the 8-period ahead returns. The bootstrapping method of forecasting in the **`rugarch`** package may be a solution, which I may tackle in another project and determine its feasibility and accuracy. I am also curious about the performance of rolling estimation since there can be structural changes to AAPL, such as business environment, company prospects, new innovations, etc. that could affect the estimation of the models.

## References

Costa, F. J. M. (2017). Forecasting volatility using GARCH models. Retrieved from <https://core.ac.uk/download/pdf/132797589.pdf>

Schmidt, L. (2021). Volatility Forecasting Performance of GARCH Models: A Study on Nordic Indices During COVID-19. Retrieved from <https://www.diva-portal.org/smash/get/diva2:1566342/FULLTEXT01.pdf>

Wennström, A. (2014). Volatility Forecasting Performance: Evaluation of GARCH type volatility models on Nordic equity indices. Retrieved from <https://www.math.kth.se/matstat/seminarier/reports/M-exjobb14/140616b.pdf>

Wikipedia. (n.d.) Autoregressive conditional heteroskedasticity. Retrieved 15 July 2022, from https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity.

Zivot, E. (2013). Univariate GARCH. Presentation, University of Washington. Retrieved from <https://faculty.washington.edu/ezivot/econ589/ch18-garch.pdf>
