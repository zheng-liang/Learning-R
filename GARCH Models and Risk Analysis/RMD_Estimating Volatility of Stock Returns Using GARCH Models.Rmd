---
title: "Estimating Volatility of Stock Returns Using GARCH Models"
author: "Tan Zheng Liang"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    code_folding: show
    highlight: tango
    theme: flatly
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center")
knitr::knit_hooks$set(purl = knitr::hook_purl)
```

## 1 Introduction

The purpose of this project was to learn about Generalized Auto-Regressive Conditional Heteroskedasticity (GARCH) models and how to implement them in R to measure volatility of stock returns. It introduces the packages in R that is used for building GARCH models and briefly touches on basic n-ahead forecasting to evaluate the models' out-of-sample forecast performance.

## 2 Packages Required

```{r load packages, message=FALSE, warning=FALSE}
library(dplyr)
library(forecast) # For ARIMA models, forecasting and evaluation
library(PerformanceAnalytics) # For portfolio performance and risk analysis
library(PortfolioAnalytics) # For portfolio optimization and analysis
library(quantmod) # For obtaining historical prices from Yahoo Finance
library(rugarch) # For univariate GARCH models
library(urca) # For unit root tests
```

## 3 Retrieve Data and Check Stationarity {.tabset .tabset-pills}

### 3.1 Data and Plots

In typical time series analysis, we usually require that the time series data is stationary with a constant mean and constant variance. However, we may have stationarity with changing conditional variance (conditional heteroskedasticity), commonly seen in financial data. In such data, the plots would show periods of volatility clustering, where there are periods of high and low volatility and variance is not constant in these periods. For instance, let us take a look at the daily returns of the SPDR S&P 500 ETF Trust (ticker: SPY), which we will be using for the rest of this project.

```{r load SPY price data}
# Load SPY price data from Yahoo Finance using quantmod package

# Set start and end dates for data retrieval
startdate <- as.Date("2020-01-01")
enddate <- as.Date("2022-07-01")

# Retrieve price data
spy <- quantmod::getSymbols(Symbols = "SPY",
                            src = "yahoo",
                            from = startdate, to = enddate,
                            periodicity = "daily",
                            auto.assign = F)

# View the first and last 4 observations
data.frame(c(head(spy, n = 4), tail(spy, n = 4)))

# Check number of observations
nrow(spy)

# Check for missing data
colSums(is.na(spy))
```

```{r calculate log returns}
# Calculate log/continuous returns using Adjusted column (column 6)

# Remove first row of calculated returns since returns cannot be calculated for first observation
r_spy <- PerformanceAnalytics::Return.calculate(prices = spy[, 6], method = "log")[-1,]
colnames(r_spy) <- "Log Returns"

# Check that first row has been removed
head(r_spy)
nrow(r_spy)

# Chart weekly log-returns over time
plot.zoo(r_spy, main = "Daily Log-Returns of AAPL", xlab = "Time", ylab = "Log-Return")
```

If we were to only look at the period in early 2020 (COVID-19 outbreak), we see that there is a period where large absolute returns are followed by large absolute returns, and subsequently there is a period of low absolute returns followed by low absolute returns. I charted the monthly volatility (measured using annualized standard deviation) of `r_spy` to better present the fluctuations in returns that might not be easily seen in the daily returns plot.

```{r rolling volatility of SPY returns}
# Plot rolling volatility of SPY returns

# width = 22 to approximately calculate annualized sd using a monthly rolling-window
PerformanceAnalytics::chart.RollingPerformance(R = r_spy, width = 22, FUN = "sd.annualized", scale = 252, main = "1-Month Rolling Volatility of SPY Returns")
```

### 3.2 Unit Root and Stationarity Tests

We can check if the series is stationary using an Augmented Dickey Fuller Test, where the null hypothesis is that the time series has a unit root.

```{r ADF test on AAPL returns}
# ADF Test with , lags selected based on AIC

r_spy %>% urca::ur.df(type = "none", selectlags = "AIC") %>% summary()
```

The test-statistic value of -17.04 against the critical value of -1.95 at the 5% significance level means that we can reject the null hypothesis.

Despite varying variances at different periods, the ADF and KPSS tests shows that the series is stationary. The problem of conditional heteroskedasticity is not addressed in ARIMA and other models that assumed constant variance. We can use GARCH models to estimate this volatility to better manage risk and improve forecasts of returns. Before discussing GARCH, it may be helpful to talk about the ARCH model since GARCH builds upon it.

## 4.1 ARCH Models {.tabset .tabset-pills}

ARCH or Auto-Regressive Conditional Heteroskedasticity models can be thought of as an autoregressive model. Suppose that the returns at time $t$ was modeled as such (remains true for all the GARCH models):

$$r_t = \mu + \varepsilon_t$$

where $\mu$ can be a constant or represented with ARMA models, and $\varepsilon_t$ is the residual at time $t$. $\varepsilon_t$ can be separated into a time-varying volatility or standard deviation $\sigma_t$ and a white noise error term (or called innovations/shocks) $z_t$ which is i.i.d. with mean of 0 and variance of 1, which we write as $\varepsilon_t = \sigma_t z_t$. 

In most cases, $z_t$ is assumed to have a normal distribution, but with the fat tails and slight skew that is typical of financial returns, it is also possible to use a Student's t-distribution or a skewed t-distribution.

```{r distribution of SPY returns}
# Plot distribution of SPY daily log returns

PerformanceAnalytics::chart.Histogram(R = r_spy,
                                      main = "Distribution of SPY Daily Log-Returns", 
                                      methods = c("add.density", "add.normal"), 
                                      colorset = c("blue", "red", "black"))

legend(x = "topleft", legend = c("Log-Return", "Density", "Normal"), col = c("blue", "red", "black"), lty = 2)
```

```{r descriptive stats of SPY returns}
# Look at skewness and kurtosis of SPY returns

PerformanceAnalytics::table.Distributions(R = r_spy)
```

An ARCH(1) model is then formulated as:

$$Var(\varepsilon_t) = \sigma_t^2 = \omega + \alpha_1 \varepsilon^2_{t-1}$$

What the formula essentially means is that the conditional variance in the current period ($\sigma_t^2$) is affected by the errors in the previous period. $\omega > 0$ since variance of first period should also be non-negative and $\alpha_1 > 0$ so that larger error terms in the previous period means higher conditional variance but $\alpha_1 < 1$ to prevent "explosive" variance. This can be extended to an ARCH(q) model where we have $q$ lags of error terms.

Moving forward, I would estimate the models using data up to 15 June 2022 and use the rest of the data to evaluate the models.

```{r split data}
# Split the data for training and testing models
trainset <- r_spy["/2022-06-15",]
nrow(trainset)

testset <- r_spy["2022-06-16/",]
nrow(testset)
```

### 4.1.1 ARCH(1) Model

I used an ARMA model to estimate the mean equation before generating the ARCH model.

```{r estimate mean equation using ARMA model}
# Try a simple ARMA model with max 1 lags of AR and MA each

meaneqn <- forecast::auto.arima(y = trainset, 
                                max.p = 1, max.q = 1, 
                                stepwise = F, approximation = F, trace = F, 
                                ic = "aic")
summary(meaneqn)
```

Based on the output, ARMA(4, 1) with zero mean (the intercept) was chosen. With this, we can create the model specification for the ARCH model using **`ugarchspec`**.

```{r create ARCH(1) model specification}
# Create ARCH(1) model specification using rugarch package, arguments to be in a list
# Check ?ugarchspec for the arguments

# Specify ARCH(1) model by setting order of GARCH terms to 0
archmodel <- list(model = "sGARCH", garchOrder = c(1, 0))

# Specify mean model as found using auto.arima
meanmodel <- list(armaOrder = c(1, 1), include.mean = F)

# Assume that the shocks, z_t, is normally distributed
arch_spec <- rugarch::ugarchspec(variance.model = archmodel, mean.model = meanmodel, distribution.model = "norm")
```

```{r fit ARCH(1) model with normal distribution}
# Fit the data to the ARCH(1) specification

fitARCH <- rugarch::ugarchfit(spec = arch_spec, data = trainset, solver = "hybrid")

fitARCH
```

The output is split into several tables. The first table `Optimal Parameters` gives us the coefficients, standard errors and significance of the coefficients for the mean and ARCH equations, as well as the robust errors. Using the values, we would write the equations:

$$\hat{r}_t = -0.61r_{t-1} + 0.49\varepsilon_{t-1}\\ \widehat{Var(\varepsilon_t)} = \hat{\sigma}_t^2 = 0.0001 + 0.67\varepsilon_{t-1}^2$$

The second table `Information Criteria` shows the different information criteria values, which can be compared across models to check which model fits the data better. The third table `Weighted Ljung-Box Test on Standardized Residuals` shows results for serial correlation in the residuals and based on the test, the null hypothesis was rejected at higher order of lags. The next table that interests us is the `Weighted ARCH LM Tests` and the low p-values means that we reject the null hypothesis (alternative is that there is ARCH effects). We may need to increase the order of ARCH terms to account for the rest of the ARCH effects. The last table `Adjusted Pearson Goodness-of-Fit Test` shows that the assumption of normally distributed errors was rejected, and we should try other distributions.

### 4.1.2 ARCH(2) Model with t-Distribution

I created an ARCH(p) model to account for remaining ARCH effects and assumed that the shocks $z_t$ have a t-distribution.

```{r ARCH(2) model with normal distribution}
# Specify an ARCH(2) model
arch2model <- list(model = "sGARCH", garchOrder = c(2, 0))

# Assume normal distribution of error term z_t
arch2std_spec <- rugarch::ugarchspec(variance.model = arch2model, mean.model = meanmodel, distribution.model = "std")

fitARCH2std <- ugarchfit(spec = arch2std_spec, data = trainset, solver = "hybrid")

fitARCH2std
```

ARCH LM Tests show that we cannot reject the null hypothesis, so we have sufficiently accounted for ARCH effects in the model. Pearson Goodness-of-Fit Test shows that the assumption of t-distribution might be rejected. Next, we try a skewed Student's t-distribution.

### 4.1.3 ARCH(2) Model with Skewed t-Distribution

I created an ARCH(2) model with a skewed t-distribution to account for the heavy tail and skewed distribution.

```{r ARCH(2) model with skewed t-distribution}
# Use skewed t-distribution

arch2sstd_spec <- ugarchspec(variance.model = arch2model, mean.model = meanmodel, distribution.model = "sstd")

fitARCH2sstd <- rugarch::ugarchfit(spec = arch2sstd_spec, data = trainset)

fitARCH2sstd
```

The ARCH(2) with skewed t-distribution has slightly higher AIC than the ARCH(2) with t-distribution. Pearson Goodness-of-Fit Test indicated that the skewed t-distribution cannot be rejected as a good fit for the shocks.

### 4.1.4 Plots of ARCH(2) Model

```{r plot ARCH(2) with t-distribution}
# Plot some data from the ARCH(2) with skewed t-distribution

par(mfrow = c(2, 2))

# Plot 1: Actual return series with conditional SD
# Plot 3: Conditional volatility vs absolute returns
# Plot 8: Distribution of residuals
# Plot 9: Q-Q plot of residuals to check for normality

plotnum <- c(1, 3, 8, 9)

for (i in plotnum) {
  plot(fitARCH2sstd, which = i)
}
```

## 4.2 Standard GARCH Models {.tabset .tabset-pills}

In order to capture the effects of volatility clustering, an ARCH model with many lags may be required. To solve this issue, the standard Generalized ARCH (sGARCH) can be used. The model can be thought of as an autoregressive moving average model, and is the basic model in the GARCH variation of models. A GARCH(1, 1) model can be written as:

$$\sigma_t^2 = \alpha_0 + \alpha_1 \varepsilon_{t-1}^2 + \beta_1 \sigma_{t-1}^2 ~, \text{ where } \alpha_0,~ \alpha_1,~ \beta_1 > 0$$

Essentially, the conditional variance in the current period is affected by the errors and the conditional variance in the previous period. Similar to ARCH, the model can be extended to a GARCH(p, q) model with $p$ lags of conditional variance and $q$ lags of error terms.

### 4.2.1 GARCH(1, 1) Model

Create an GARCH(1, 1) model specification with normally distributed $z_t$:

```{r GARCH(1,1) with normal distribution}
# Specify GARCH(1,1) model 
garchmodel <- list(model = "sGARCH", garchOrder = c(1, 1))

# Assume a normal distribution of the error term z_t
garch_spec <- rugarch::ugarchspec(variance.model = garchmodel, mean.model = meanmodel, distribution.model = "norm")

# Fit GARCH(1, 1)
fitGARCH <- rugarch::ugarchfit(spec = garch_spec, data = trainset, solver = "hybrid")

fitGARCH
```

With a GARCH(1, 1) model, the coefficient of the GARCH term $\beta_1$ is statistically significant (but $\omega$ is insignificant at the 5% confidence level), standardized residuals are not serially correlated and there are no ARCH effects left after including the GARCH term. The Adjusted Pearson Goodness-of-Fit Test statistic indicates that the normal distribution is rejected again. We can write the formula as:

$$\hat{r}_t = -0.58r_{t-1} + 0.52\varepsilon_{t-1} \\ \widehat{Var(\varepsilon_t)} = \hat{\sigma}_t^2 = 0.25 \varepsilon_{t-1}^2 + 0.73 \sigma_{t-1}^2$$

I have also estimated a GARCH(1, 1) with the assumption that $z_t$ follows a t-distribution and a skewed t-distribution.

### 4.2.2 GARCH(1, 1) Model with t-Distribution

```{r GARCH(1,1) with t-distribution}
# GARCH(1, 1) with errors that follow a t-distribution

garchstd_spec <- rugarch::ugarchspec(variance.model = garchmodel, mean.model = meanmodel, distribution.model = "std")

fitGARCHstd <- rugarch::ugarchfit(spec = garchstd_spec, data = trainset, solver = "hybrid")

fitGARCHstd
```

Pearson Goodness-of-Fit Test indicates again that the shocks may not follow a t-distribution.

### 4.2.3 GARCH(1, 1) Model with Skewed t-Distribution

```{r GARCH(1,1) with skewed t-distribution}
# GARCH(1, 1) with errors that follow a skewed t-distribution

garchsstd_spec <- rugarch::ugarchspec(variance.model = garchmodel, mean.model = meanmodel, distribution.model = "sstd")

fitGARCHsstd <- rugarch::ugarchfit(spec = garchsstd_spec, data = trainset, solver = "hybrid")

fitGARCHsstd
```

Assuming that $z_t$ follows a skewed t-distribution results in serially correlated residuals.

### 4.2.4 Plots of GARCH(1, 1) Model

```{r plot GARCH(1,1) with t-distribution}
# Plot some data from the GARCH(1, 1) with skewed t-distribution
# similar to what was done in the ARCH section

par(mfrow = c(2, 2))

for (i in plotnum) {
  plot(fitGARCHsstd, which = i)
}
```

## 5 Evaluating Forecast Performance {.tabset .tabset-pills}

In this section, I evaluated the out-of-sample forecast performance of the ARCH and GARCH models created in the previous sections. I only looked at the models that assumed $z_t$ follows the t- or skewed t-distribution for simplicity.

However, before jumping into the forecast, there is a need to highlight the difficulty of evaluating GARCH models. The difficulty lies in using an appropriate proxy of volatility, since it is not observable at a point in time. The paper by Wennström (2014) describes some proxies of volatility, such as squared returns, using moving averages of variance and cumulative squared intra-day returns. In the paper, and another by Schmidt (2021), the High-Low Range proxy was used and a simple variation of the formula is given by:

$$v_t = \ln \bigg( \frac{H_t}{L_t} \bigg)$$

I extracted the daily high and daily low prices of SPY in the test period and calculate the volatility proxy.

```{r high low price}
# Extract high (column 2) and low (column 3) prices within test period

proxy <- spy["2022-06-16/", 2:3]

proxy$Volatility <- log(proxy[, 1] / proxy[, 2])

data.frame(proxy)
```

In the following sub-sections, I evaluated the models against the volatility proxy using Mean Squared Error, Root Mean Squared Error and Mean Absolute Error, grouped by the model types.

The formula for each evaluation metric is:

$$
MSE = \frac{1}{n} \sum_{n=1}^N (v_{n+1}^2 - \hat{\sigma}_{n+1}^2)^2 \\
RMSE = \sqrt{MSE} \\
MAE = \frac{1}{n} \sum_{n=1}^N |v_{n+1}^2 - \hat{\sigma}_{n+1}^2|
$$

### 5.1 ARCH Forecasts

```{r forecast ARCH(2) models}
# Forecast ARCH(2) with t-distribution 10 periods ahead
# (number of observation in month of June)
ARCH2std_fcst <- rugarch::ugarchforecast(fitORspec = fitARCH2std, n.ahead = 10)

# Forecast ARCH(2) with skewed t-distribution 10 periods ahead
ARCH2sstd_fcst <- rugarch::ugarchforecast(fitORspec = fitARCH2sstd, n.ahead = 10)
```

Store forecasted conditional standard deviation of ARCH models into a data frame:

```{r store forecast of ARCH}
fcstARCH2 <- cbind(sigma(ARCH2std_fcst), sigma(ARCH2sstd_fcst)) %>%
  `colnames<-`(c("ARCH2_STD", "ARCH2_SSTD"))

data.frame(fcstARCH2)
```

Calculate evaluation metrics for each ARCH model:

```{r evaluation metrics for ARCH(2) models}
evalARCH <- NULL

for (i in 1:2) {
  mse <- mean((proxy$Volatility^2 - fcstARCH2[,i]^2)^2)
  
  rmse <- sqrt(mse)
  
  mae <- mean(abs(proxy$Volatility^2 - fcstARCH2[,i]^2))
  
  evalARCH <- rbind(evalARCH, c(MSE = mse, RMSE = rmse, MAE= mae))
}

rownames(evalARCH) <- colnames(fcstARCH2)

evalARCH
```

### 5.2 GARCH Forecasts

```{r forecast GARCH(1,1) models}
# Forecast GARCH(1, 1) with t-distribution 10 periods ahead
GARCHstd_fcst <- rugarch::ugarchforecast(fitORspec = fitGARCHstd, n.ahead = 10)

# Forecast GARCH(1, 1) with skewed t-distribution 10 periods ahead
GARCHsstd_fcst <- rugarch::ugarchforecast(fitORspec = fitGARCHsstd, n.ahead = 10)
```

Store forecasted conditional standard deviation of GARCH models into a data frame:

```{r store forecast of GARCH}
fcstGARCH <- cbind(sigma(GARCHstd_fcst), sigma(GARCHsstd_fcst)) %>%
  `colnames<-`(c("GARCH11_STD", "GARCH11_SSTD"))

data.frame(fcstGARCH)
```

Calculate evaluation metrics for each GARCH model:

```{r evaluation metrics for GARCH(1,1) models}
evalGARCH <- NULL

for (i in 1:2) {
  mse <- mean((proxy$Volatility^2 - fcstGARCH[,i]^2)^2)
  
  rmse <- sqrt(mse)
  
  mae <- mean(abs(proxy$Volatility^2 - fcstGARCH[,i]^2))
  
  evalGARCH <- rbind(evalGARCH, c(MSE = mse, RMSE = rmse, MAE= mae))
}

rownames(evalGARCH) <- colnames(fcstGARCH)

evalGARCH
```

### 5.3 Plots of Forecast

```{r plotting forecasts from models}
par(mfrow = c(2,2))

plot(fcstARCH2[,1], type = "l", main = "Forecast of ARCH(2) with t-Distribution",
     xlab = "Period", ylab = "Conditional SD")
plot(fcstARCH2[,2], type = "l", main = "Forecast of ARCH(2) with skewed \n t-Distribution",
     xlab = "Period", ylab = "Conditional SD")

plot(fcstGARCH[,1], type = "l", main = "Forecast of GARCH(1,1) with t-Distribution",
     xlab = "Period", ylab = "Conditional SD")
plot(fcstGARCH[,2], type = "l", main = "Forecast of GARCH(1,1) with skewed \n t-Distribution",
     xlab = "Period", ylab = "Conditional SD")
```

By plotting the forecasts, we see that the GARCH models do not have the ups and downs as shown in the ARCH models. However, these models do not seem to be able to forecast the volatility of SPY returns sufficiently even in the very short run. This may not be a surprise as many factors affect volatility, for example news related to the company and macroeconomic environment.

## 6 Final Remarks

While ARCH and GARCH can be used to model conditional variance, I believe there is much to be desired from the out-of-sample predictive ability from the models in this project. Furthermore, as $z_t$ is a stochastic variable, I was unable to forecast the returns series and it would require certain bootstrapping techniques to do so. There are also other variants of GARCH models that may be useful for modeling volatility, which I will explore in the future.

## References

Costa, F. J. M. (2017). Forecasting volatility using GARCH models. Retrieved from <https://core.ac.uk/download/pdf/132797589.pdf>

Schmidt, L. (2021). Volatility Forecasting Performance of GARCH Models: A Study on Nordic Indices During COVID-19. Retrieved from <https://www.diva-portal.org/smash/get/diva2:1566342/FULLTEXT01.pdf>

Wennström, A. (2014). Volatility Forecasting Performance: Evaluation of GARCH type volatility models on Nordic equity indices. Retrieved from <https://www.math.kth.se/matstat/seminarier/reports/M-exjobb14/140616b.pdf>

Wikipedia. (n.d.) Autoregressive conditional heteroskedasticity. Retrieved 15 July 2022, from https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity.

Zivot, E. (2013). Univariate GARCH. Presentation, University of Washington. Retrieved from <https://faculty.washington.edu/ezivot/econ589/ch18-garch.pdf>
