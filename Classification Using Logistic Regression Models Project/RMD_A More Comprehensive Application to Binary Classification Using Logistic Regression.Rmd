---
title: "A More Comprehensive Application to Binary Classification Using Logistic Regression"
author: "zhengliang"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(purl = knitr::hook_purl)
```

## 1 Binary Classification Using Other Dataset With Out-of-Sample Evaluation

In the first project of Binary Classification Using Logistic Regression Models (click [here](https://tinyurl.com/c276mvp7) to access it), I explored the basics of logistic regression using the **`glm`** function on the `Pima Indians Diabetes` dataset. However, real world data sets are likely to have more attributes/variables which we might not be able to select variables simply based off p-values to indicate significance of variables. Another possible problem was the usefulness of the model to predict outcomes on out-of-sample data.

I sought to address these problems in this section by applying more real-world techniques and comparing the predictive power of the different models. However, one should remember that the purpose of machine learning and statistical modelling are different. Machine learning usually aims for best prediction while statistical modelling might be more towards the inference of explanatory variables to the response variable.

## 2 Packages Required

```{r load packages, message=FALSE, warning=FALSE}
# Use install.packages("packagename") if the packages are not installed in R

# Basic packages
library(gridExtra) # Plot and arrange multiple graphs in a single pane
library(tidyverse) # For ggplot and dplyr packages
library(readxl) # Fore reading Excel files
library(stargazer) # For neat tables and outputs, where possibles

# For machine learning, logistic regression, plots and tests
library(car) 
library(caret) # For building ML models
library(corrplot) # For correlation plot
library(mlbench) # For data sets
library(lmtest) # For statistical tests
library(glmnet) # For LASSO logistic regression
library(ROCR) # For determining cutoff-probability using ROC


# For dealing with imbalanced data sets
library(ROSE) # For under- and over-sampling methods
library(smotefamily) # For SMOTE methods
```

## 3 Dataset and Exploratory Data Analysis

The dataset that I used is called [`default of credit card clients`](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients), which can be found in the UCI Machine Learning Repository. It contains customer credit default data from Taiwan in 2005. Other than the binary classification of default and non-default, the probability of default may also be an important result. The file downloaded is an Excel file and to load it into R, we need the **`readxl`** package.

```{r load dataset from wd, message=FALSE}
# This step loads the file from the working directory. If unsure of wd, use 
# getwd() to check the location of the wd and setwd() to set the wd where the file is saved.

default.data <- data.frame(read_excel("credit card default.xls"))
```

Let us take a peek at `default.data`.

```{r head tail}
# Show only the first 10 columns as there are too many variables

stargazer(rbind(head(default.data[,1:10]), tail(default.data[,1:10])),
          type = "text",
          title = "First and Last 6 Observations in Default Data",
          summary = F)

colnames(default.data)
```

From the **`head`** function, we saw that the first row should be the column names of the data frame and the first column should be the row names. There are many variables, 24 to be exact, with 1 dependent variable (Y, the last column) and 23 independent variables (X1 to X23). I would start by adjusting the column and row names.

```{r adjust column names}
colnames(default.data) <- default.data[1,]

default.data <- default.data[-1,]
```

```{r adjust row names}
# Remove the first column since ID matches the auto-assigned row numbers.
default.data <- default.data[,-1]

rownames(default.data) <- NULL # Reset the row numbers

stargazer(head(default.data[,1:10]),
          type = "text",
          title = "First 6 Observations in Default Data, Adjusted",
          summary = F)
```

Next, I would change the some of the column names to be something more descriptive but it is optional.

```{r renaming columns}
newnames <- c("PAY_SEP","PAY_AUG","PAY_JUL", "PAY_JUN", "PAY_MAY", "PAY_APR",
              "BILLAMT_SEP","BILLAMT_AUG","BILLAMT_JUL", "BILLAMT_JUN", "BILLAMT_MAY", "BILLAMT_APR",
              "PAYAMT_SEP","PAYAMT_AUG","PAYAMT_JUL", "PAYAMT_JUN", "PAYAMT_MAY", "PAYAMT_APR")

oldnames <-names(default.data %>%
                   dplyr::select(grep("PAY" , names(default.data)), 
                                 grep("BILL", names(default.data)))) 

colnames(default.data)[colnames(default.data) %in% oldnames] <- newnames

default.data <- default.data %>%
  rename(default = `default payment next month`)

colnames(default.data)
```

I have listed the description of the variables in the table below, which can be found in the UCI Machine Learning Repository where the data set was downloaded from.

| Variable                  | Description                                                                                                                                        |
|------------------------|-----------------------------------------------|
| LIMIT_BAL                 | Amount of the given credit, including both the individual consumer credit and his/her family (supplementary) credit                                |
| SEX                       | Gender. Male = 1, Female = 2                                                                                                                       |
| EDUCATION                 | Graduate School = 1, University = 2, High School = 3, Others = 4                                                                                   |
| MARRIAGE                  | Married = 1, Single = 2, Others = 3                                                                                                                |
| AGE                       | In years                                                                                                                                           |
| PAY_SEP - PAY_APR         | History of past payment from April to September 2005. Pay duly = -1, Delay one month = 1, Delay two months = 2, ..., Delay nine months or more = 9 |
| BILLAMT_SEP - BILLAMT_APR | Amount of bill statement from April to September 2005.                                                                                             |
| PAYAMT_SEP - PAYAMT_APR   | Amount of previous payment.                                                                                                                        |
| default                   | Response variable. Yes = 1, No = 0.                                                                                                                |

Now, we can begin exploring the data set through an Exploratory Data Analysis.

```{r structure}
str(default.data)
```

We can see that all the data values has been loaded as characters, and we need to change this to their appropriate types (numeric, factors, etc.).

```{r changing value types}
# Changing variables PAYAMT, BILLAMT, AGE and LIMIT_BAL to numeric
default.data <- default.data %>% 
  mutate(
    across(
      c(grep("PAYAMT", names(default.data)), 
        grep("BILLAMT", names(default.data)), 
        "AGE", "LIMIT_BAL"), 
      as.numeric))

# Changing variables SEX, EDUCATION, MARRIAGE and default to factor
default.data <- default.data %>%
  mutate(
    across(
      c("SEX", "EDUCATION", "MARRIAGE", "default",
        grep("PAY_", names(default.data))),
      as.factor))
```

```{r summary}
summary(default.data)
```

We can see that there are some variables that stood out. For example, in the variables *`EDUCATION`* and *`MARRIAGE`*, there are other classes that were not known beforehand. The variables *`PAY_`* has values of -2 and 0, which were also not indicated in the description of the data. This could probably have meant prepayments and paying within the month respectively, and since they could provide information about how paying habits affect whether a person defaulted, we should not remove these observations but keep in mind that there is ambiguity in the data set.

Let us clean the data for variables *`EDUCATION`* and *`MARRIAGE`*. Since they have each have a class called `Others`, we can consider replacing the unknown classes with this class.

```{r replace unknown class with others}
# For EDUCATION
levels(default.data$EDUCATION)[levels(default.data$EDUCATION) %in% c("0", "5", "6")] <- "4"

# For MARRIAGE
levels(default.data$MARRIAGE)[levels(default.data$MARRIAGE) == "0"] <- "3"

default.data %>% select(where(is.factor)) %>% str()
```

After the cleaning of the *`EDUCATION`* and *`MARRIAGE`* variables, we can see that only 4 and 3 levels are left, from the original 7 and 4 respectively. The *`PAY_`* variables have different number of levels, which we can check when we further analyze the variables. We can plot out the variables in a univariate and multivariate plots to visualize how they are distributed and correlate with one another.

### 3.1 Univariate Analysis

We can first take a look at the distribution of the *`default`* variable.

```{r dist of default, fig.align="center"}
ggplot(data = default.data, aes(x = default, fill = default)) +
  geom_bar()
```

It can be seen that this is an imbalanced data set, with much higher number of non-default observations compared to default observations. We would need to correct these using under-sampling, over-sampling or SMOTE methods, and we can compare their in-sample and out-of-sample predictions against uncorrected data. 

I have included plots of the other factor variables below. Using the package **`gridExtra`** allows us to arrange a number of plots into a single page.

```{r dist of sex_educ_marr_age, message=FALSE, fig.align="center"}
# For categorical and discrete variables
sex <- ggplot(data = default.data) +
  geom_bar(aes(x = SEX, fill = default)) +
  theme(legend.title = element_text(size = 10),
        legend.key.size = unit(0.5, "cm"),
        axis.title.x = element_text(size = 10))

education <- ggplot(data = default.data) + 
  geom_bar(aes(x = EDUCATION, fill = default)) +
  theme(legend.title = element_text(size = 10),
        legend.key.size = unit(0.5, "cm"),
        axis.title.x = element_text(size = 10))

marriage <- ggplot(data = default.data) +
  geom_bar(aes(x = MARRIAGE, fill = default)) +
  theme(legend.title = element_text(size = 10),
        legend.key.size = unit(0.5, "cm"),
        axis.title.x = element_text(size = 10))

age <- ggplot(data = default.data) +
  geom_histogram(aes(x = AGE, fill = default)) +
  theme(legend.title = element_text(size = 10),
        legend.key.size = unit(0.5, "cm"),
        axis.title.x = element_text(size = 10))

gridExtra::grid.arrange(sex, education, marriage, age, ncol = 2, nrow = 2)
```

For the *`PAY_`* variables, I have used a contingency table as there are several of these variables with multiple levels, making plots difficult to see.

```{r contingency table}
colno <- grep("PAY_", colnames(default.data))

cont_tab <- list()

for (i in colno) {
  temp_tab <- table(default.data$default, default.data[,i])
  
  name <- colnames(default.data)[i]
  
  cont_tab[[name]] <- temp_tab
}

cont_tab
```

The contingency table shows an interesting observation, where *`PAY_MAY`* and *`PAY_APR`* do not have the factor class of "1", explaining the difference in the number of levels in the *`PAY_`* variables. However, there are too many levels and we can see that typically levels -2, -1 and 0 have the most observations, followed by 1, 2 and 3. Levels 4 to 8 appears to have few observations. We may want to reclassify the levels into just these 3 groups as more variables generally affects variance of our estimators and the reliability of any statistical tests performed.

From the plots, it is hard to determine if the *`default`* variable is correlated with gender, education, marriage and age. This is because while we see higher default in certain groups, those groups also had more observations which would allow us to find more people who defaulted.

```{r dist billamt and payamt, fig.align="center"}
# For continuous variables
billamt <- default.data %>%
  select(grep("BILLAMT", names(default.data)))

payamt <- default.data %>%
  select(grep("PAYAMT", names(default.data)))

par(mfrow = c(1, 6), mar = c(1, 3, 3, 2))
for (i in 1:6) {
  boxplot(billamt[,i], main = names(billamt)[i])
}

par(mfrow = c(1, 6), mar = c(1, 3, 3, 2))
for (i in 1:6) {
  boxplot(payamt[,i], main = names(payamt)[i])
}
```

From the boxplots, we can tell that the continuous variables are not normally distributed as they have very long tails (a lot of outlier points).

### 3.2 Multivariate Analysis

Let us visualize the correlation between the numeric variables in `default.data`.

```{r corrplot, fig.align="center"}
# Calculate the correlation between continuous variables
correl <- default.data %>% 
  select(where(is.numeric)) %>%
  cor(method = "spearman")

# Plot the correlation in a heatmap
corrplot(corr = correl,
         method = "color",
         type = "lower",
         main = "Correlation Matrix of Continuous Variables in Default Data",
         mar = c(0, 0, 1.5, 0))
```

We can see that the correlation between *`BILLAMT_`* variables is very high, which is likely due to the time component involved in these variables. We could expect that if the bill statement in April is high, then the bill statement in May would likely be high as well. However, it seemed that the correlation between the *`PAYAMT_`* is rather moderate. For the correlation between *`BILLAMT_`* and *`PAYAMT_`*, there seems to be a pattern of relatively higher correlation on the diagonals, i.e. between *`BILLAMT_AUG`* and *`PAYAMT_SEP`* or *`BILLAMT_JUL`* and *`PAYAMT_AUG`*. Due to the high correlation between *`BILLAMT_`* variables, I may need to create new features to eliminate the problem multicollinearity.

### 3.3 Creating New Variables

Since the correlation between *`BILLAMT_`* variables are very high, I simplified the information with them using the average utilization over the period of April to September. Utilization would be obtained from an observation's mean *`BILLAMT_`* variables divided by its *`LIMIT_BAL`*. A higher utilization may signal a higher probability of default.

```{r avg and max util}
# Calculate average bill using rowMeans function
avg_util <- round((rowMeans(billamt)/ default.data$LIMIT_BAL)*100, 2)
```

I have bound the variables of interest into a new data frame called `new_data`.

```{r bind variables}
new_data <- data.frame(default.data[,1:11],
                       default.data[, grep("PAYAMT", names(default.data))],
                       avg_util,
                       default = default.data$default)

stargazer(rbind(head(new_data), tail(new_data)), 
          type = "text", 
          title = "First and Last 6 Observations in New Data", 
          summary = F,
          digits = 2)
```

The variables *`PAY_`* has too many classes, and there would be a need to reduce them. As seen in the contingency table produced in Section 3.1 for the *`PAY_`* variables, I have reduced the original 11 classes into 3. 0 = those who paid duly/early , 1 = those who delayed payment for one to three months, and 2 = those who delayed for more than three months.

```{r changing classes}
# Set level 0 for those who were originally given levels -2, -1 and 0

for (i in colno) {
  levels(new_data[,i])[levels(new_data[,i]) %in% c("-2", "-1", "0")] <- "0"
  
  levels(new_data[,i])[levels(new_data[,i]) %in% c("1", "2", "3")] <- "1"
  
  levels(new_data[,i])[!levels(new_data[,i]) %in% c("-2", "-1", "0", "1", "2", "3")] <- "2"
}

levels(new_data$PAY_SEP) # To check as an example
```

Furthermore, the variables *`LIMIT_BAL`* and *`PAYAMT_`* have very large scales, which could affect the coefficients, so I decided to use units of $1,000 instead.

```{r scaling LIMIT_BAL avg_pmt and pmt_bill_diff}
new_data$LIMIT_BAL <- new_data$LIMIT_BAL / 1000

new_data[, grep("PAYAMT", names(new_data))] <- new_data[, grep("PAYAMT", names(new_data))] / 1000

summary(new_data)
```

I calculated the correlation and plotted the correlation matrix to see if the numeric variables in `new_data` data frame are highly correlated.

```{r correl new var, fig.align="center"}
correl <- new_data %>% 
  select(where(is.numeric)) %>%
  cor(method = "spearman")

corrplot(corr = correl,
         method = "color",
         type = "lower",
         addCoef.col = "black",
         main = "Correlation Matrix of Continuous Variables in New Data",
         mar = c(0, 0, 1.5, 0))
```

From the correlation matrix, there is no correlation between variables higher than 0.7, which may indicate that multicollinearity is not a problem, which we can check with the variance inflation factor later.

## 4 Building the Models

Now that I have settled on the variables to be used for the logistic regression model, I can start to built the different models that are commonly used. Before modelling, I would split the data into training and testing sets in a 80:20 split.

```{r split data}
# Split data into 80% training and 20% testing sets
set.seed(1)
validation <- createDataPartition(new_data$default, p = 0.80, list = F)

traindata <- new_data[validation,]

testdata <- new_data[-validation,]
```

### 4.1 Full Model

I would start with building a full model that contains all the explanatory variables from the new data set created in Section 3.3, which can be used as a benchmark for evaluation.

```{r full model}
full.model <- glm(default == "1" ~ .,    # 1 refers to those who default
                  data = traindata,
                  family = binomial(link = "logit"))

stargazer(full.model, 
          type = "text", 
          title = "Regression Output of Full Model")
```

I used the model to calculate the variance inflation factor of each variable to test for multicollinearity. It can be seen that none of the variables has a large generalized VIF value, so multicollinearity may not be a problem.

```{r vif2}
car::vif(full.model)
```

### 4.2 Restricted Model

From the regression results of the `full.model`, both of the *`MARRIAGE`* factor variables and *`PAYAMT_`* variables from April to July seemed to be statistically insignificant, so I removed them while keeping the rest of the variables in the restricted model.

```{r rest model wo marriage}
# Remove marriage variables
rest.model <- glm(default == "1" ~ .-MARRIAGE -PAYAMT_APR -PAYAMT_MAY -PAYAMT_JUN -PAYAMT_JUL,
                   data = traindata,
                   family = binomial(link = "logit"))

stargazer(rest.model, 
          type = "text", 
          title = "Regression Output of Restricted Model")
```

I did a likelihood ratio test and found that the full model is preferred since the p-values showed strong evidence against the null hypothesis.

```{r lrtest2}
lmtest::lrtest(full.model, rest.model) # Result: Use full model
```

### 4.3 Checking for Influential Outliers in the Full Model

I checked for highly influential observations using Cook's Distance and outliers using standardized and studentized residuals.

```{r cooksd}
# Influence using Cook's Distance
plot(full.model, which = 4)

head(sort(cooks.distance(full.model), decreasing = T))
```

```{r standardized resid}
# Outliers with standardized/studentized residuals
std.resid <- rstandard(full.model)

val <- abs(std.resid) > 3

table(val)

val[val == "TRUE"] # Can be used to further investigate the observations
```

```{r studentized resid2}
car::outlierTest(full.model) # No outliers detected by studentized residuals
```

There was 1 influential outlier detected by the standardized residuals, which could be further investigated if necessary.

```{r looking at influential outlier}
# Use this command to get data on the influential outlier(s)
traindata[val == "TRUE",]
```

### 4.4 Checking Linearity Assumption of Full Model

To check the linearity assumption, I first found the fitted values (or the probability) using the full model. Next, I calculated the log-odds (or logit), and finally plotted the log-odds against the continuous variables in my model.

```{r fitvalues and logit}
prob.fullmod <- fitted.values(full.model)

logit = log(prob.fullmod/(1-prob.fullmod))
```

```{r plot, fig.align="center", message=FALSE, warning=FALSE}
p1 <- ggplot(data = traindata, aes(x = logit, y = LIMIT_BAL)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

p2 <- ggplot(data = traindata, aes(x = logit, y = AGE)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

p3 <- ggplot(data = traindata, aes(x = logit, y = avg_util)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

p4 <- ggplot(data = traindata, aes(x = logit, y = PAYAMT_SEP)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

p5 <- ggplot(data = traindata, aes(x = logit, y = PAYAMT_AUG)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

p6 <- ggplot(data = traindata, aes(x = logit, y = PAYAMT_JUL)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2, nrow = 3)
```

It would seem that the continuous variables are mostly linear to the log-odds, maybe other than for the *`LIMIT_BAL`* variable.

## 5 Stepwise Model Using Backward Elimination

Most of the times, when there are too many variables even after manual selection by understanding business and data interactions, we may have to use some techniques to help us select variables/features. One of these techniques is the use of step-wise models, using either forward selection or backward elimination or a combination of both. 

These techniques selects variables that are usually statistically significant or meet certain requirements, such as lowest AIC or BIC. However, there are many arguments online and from research articles against using such techniques. But it does provide a quick and easy way to get a model from multiple explanatory variables and we can use it as a benchmark. I would use the backward elimination method, but the code is similar for the other methods.

Backward elimination starts off with a model that contains all the explanatory variables and removes them one at a time to reduce AIC, with the variable that causes the largest reduction to be removed first. Before that, let us create a null model that does not use any explanatory variables.

```{r null model}
# Create a null model that predicts using grand mean
null.model <- glm(default == "1" ~ 1,
                  data = traindata,
                  family = binomial(link = "logit"))

stargazer(null.model,
          type = "text",
          title = "Regression Output of Null Model")
```

```{r bwd model}
bwd.model <- step(full.model,
                  scope = list(lower = null.model, upper = full.model),
                  direction = "backward", 
                  k = 2,
                  trace = T)
# Indicate scope argument to define range of models to examine
# Indicate direction = "backward" for backward elimination, "forward" for forward selection
# and "both" for stepwise regression
# Indicate k = 2 for AIC, k = log(n) for BIC
# Indicate trace = F if want to suppress the running of the step by step regression

stargazer(bwd.model,
          type = "text",
          title = "Regression Output of Backward Elimination Model")
```

Based on the backward elimination approach, the some of the *`PAYAMT_`* variables have been removed to achieve the lowest AIC value, although the difference is very small.

## 6 LASSO Logistic Model

Another method of selecting explanatory variables is the LASSO regression, which shrinks the coefficients of variables that are not important to zero. This can be done in R using the **`glmnet`** package.

### 6.1 Converting Data Frame to Matrix

Before we can run the lasso logistic regression, the data frame has to be converted into a matrix for **`glmnet`** to run. We also need to convert categorical variables into suitable dummy variables. All this can be done using the **`model.matrix`** function.

```{r model.matrix}
x_var <- model.matrix(default ~ ., traindata)[,-1] # Remove intercept column

colnames(x_var) # Does not include the y variable

y_var <- ifelse(traindata$default == "1", 1, 0)
```

### 6.2 Calculating lambda

For LASSO regression, there is a need to specify the lambda value, which is the amount of the penalty added to the log-likelihood function. The optimal lambda value can be obtained using the **`cv.glmnet`** function. I have chosen to use the `lambda.1se` value which would give a simpler model, but `lambda.min` can also be chosen to obtain a more complex model.

```{r lambda}
set.seed(2)
lasso.lambda <- glmnet::cv.glmnet(x = x_var, 
                                  y = y_var, 
                                  alpha = 1,
                                  family = "binomial",
                                  type.measure = "mse")

# alpha = 1 for lasso, family = "binomial" for logistic regression
# type.measure = "mse" to find lambda that optimize mean squared error, 
# "class" can also be used instead to optimize mis-classification error
```

```{r optimal lambda}
# optimal value of lambda
opt.lambda <- lasso.lambda$lambda.1se

opt.lambda
```

### 6.3 Fitting the LASSO Logistic Model

Now that the lambda value has been obtained and stored, the LASSO logit regression model can be fitted. Unfortunately, the **`summary`** function does not show the coefficients like the models fitted using **`glm`**. To extract the intercept and coefficients, use **`coef`** function on the model created.

```{r lasso model}
lasso.model <- glmnet(x_var,
                      y_var,
                      alpha = 1,
                      family = "binomial",
                      lambda = opt.lambda)

coef(lasso.model) # intercept and coefficients
```

We can see that the LASSO model has automatically shrunk the coefficients of some variables, though it might in some cases be hard to interpret. For example, it removed the coefficients for the *`PAY_AUG2`* and *`PAY_JUL2`* variables which are part of the factors of *`PAY_AUG`* and *`PAY_JUL`*. We would not be able to interpret the effect of these factor variables on the response variable, if interpretation of independent variables on the response variable was the purpose of modelling.

## 7 Evaluation of Models In-Sample

For the evaulation of the models, I would use the Receiver Operating Characteristic (ROC) and Precision-Recall Curves to choose the probability threshold. Since false negatives (predicting non-default when they have defaulted) is likely to be more costly than false positives (predicting default when they have not defaulted), the Recall score may be better than Accuracy or Precision scores.

For the ROC curve, I would aim for a false positive rate of less than 0.2. As for the Precision-Recall curve, I would like a Recall rate of around 0.6.

### 7.1 Full Model

```{r prob using full model}
# Returns the probability of default
isres_fullmodel <- predict(full.model, traindata, type = "response")
```

```{r ROC and plot}
# First argument: predicted probability, Second argument: observed response
ROCRpred <- ROCR::prediction(predictions = isres_fullmodel, 
                             labels = traindata$default,
                             label.ordering = c("0", "1"))

# Performance based on true positive rate and false positive rate
ROCperf <- ROCR::performance(prediction.obj = ROCRpred,
                             measure = "tpr",
                             x.measure = "fpr")

# Plot ROC and indicate threshold values, with a vertical line at 0.2
plot(ROCperf, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,1))

abline(v = 0.2)
```

```{r prec_recall plot}
# Performance based on precision and recall
prec_rec <- ROCR::performance(prediction.obj = ROCRpred, 
                              measure = "prec", 
                              x.measure = "rec")

# Plot Precision-Recall and indicate threshold values, with a vertical line at 0.6
plot(prec_rec, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,-0.5))

abline(v = 0.6)
```

```{r conMat fullmodel is}
# Use 0.2 as threshold
ispred.fullmodel <- as.factor(ifelse(isres_fullmodel > 0.2, "1", "0"))

caret::confusionMatrix(data = ispred.fullmodel, 
                       reference = traindata$default, 
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

Using `fullmodel` on the in-sample data, 14978 observations were correctly predicted as non-default and 3168 were correctly predicted as default. Since default (value = 1) is the positive class, there were 2141 false negatives and 3714 false positives. In other words, the model incorrectly predicted 2141 actual defaults as non-defaults. This could have serious implications to a financial institution depending on the size of the credit. The 3714 actual non-defaults predicted as defaults would also have implications in the form of higher monitoring costs, or financial institutions might reject giving credit to borrowers who are unlikely to default.

The interpretation of the confusion matrix for other models in this section is similar and so I would not go into details, but I would give a summary at the end.

### 7.2 Backward Elimination Model

```{r ROC stepmodel is}
isres_stepmodel <- predict(bwd.model, traindata, type = "response")

# First argument: predicted probability, Second argument: observed response
ROCRpred <- ROCR::prediction(predictions = isres_stepmodel, 
                             labels = traindata$default, 
                             label.ordering = c("0", "1"))

# Performance based on true positive rate and false positive rate
ROCperf <- ROCR::performance(prediction.obj = ROCRpred,
                             measure = "tpr",
                             x.measure = "fpr")

# Plot ROC and indicate threshold values, with a vertical line at 0.2
plot(ROCperf, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,1))

abline(v = 0.2)
```

```{r prec_rec stepmodel is}
# Performance based on precision and recall
prec_rec <- ROCR::performance(prediction.obj = ROCRpred, 
                              measure = "prec", 
                              x.measure = "rec")

# Plot Precision-Recall and indicate threshold values, with a vertical line at 0.6
plot(prec_rec, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,-0.5))

abline(v = 0.6)
```

```{r conMat stepmodel is}
# Use 0.2 as threshold
ispred.stepmodel <- as.factor(ifelse(isres_stepmodel > 0.2, "1", "0"))

caret::confusionMatrix(data = ispred.stepmodel, 
                       reference = traindata$default, 
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

### 7.3 LASSO Logistic Model

```{r ROC lasso is}
isres_lassomodel <- predict(lasso.model, x_var, type = "response")

ROCRpred <- ROCR::prediction(predictions = isres_lassomodel, 
                             labels = factor(y_var), 
                             label.ordering = c("0", "1"))

# Performance based on true positive rate and false positive rate
ROCperf <- ROCR::performance(prediction.obj = ROCRpred,
                             measure = "tpr",
                             x.measure = "fpr")

# Plot ROC and indicate threshold values, with a vertical line at 0.2
plot(ROCperf, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,1))

abline(v = 0.2)
```

```{r prec_rec lasso is}
# Performance based on precision and recall
prec_rec <- ROCR::performance(prediction.obj = ROCRpred, 
                              measure = "prec", 
                              x.measure = "rec")

# Plot Precision-Recall and indicate threshold values, with a vertical line at 0.6
plot(prec_rec, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,-0.5))

abline(v = 0.6)
```

```{r conMat lasso is}
## Use 0.2 as threshold
ispred.lasso <- as.factor(ifelse(isres_lassomodel > 0.2, "1", "0"))

caret::confusionMatrix(data = ispred.lasso, 
                       reference = factor(y_var),
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

The LASSO logistic regression model has led to a higher accuracy than the full model and the stepwise model. However, there is an increase in the mis-classification of actual defaults as non-defaults.

### 7.4 Summary

We can see that the models were not very reliable in identifying the actual defaults using any of the logistic regression methods. I have summarized the ranking of the models in predicting in-sample class according to different evaluation metrics in a table.

| Metric      | Ranking (In-Sample)     |
|-------------|-------------------------|
| Accuracy    | Full < Stepwise < LASSO |
| Precision   | Full < Stepwise < LASSO |
| Recall      | LASSO < Stepwise < Full |
| Specificity | Full < Stepwise < LASSO |

This is because of the highly imbalanced class observations, and there would be heavy bias towards the non-default observations. I deal with this in Section 9.

## 8 Evaluation of Models Out-of-Sample

The code used here are similar to what was used in Section 7, but in the **`predict`** function, we replace the `traindata` with `testdata`. The cut-off probability remains unchanged for any of the models.

### 8.1 Full Model

```{r conMat fullmodel oos}
# Change to testdata instead for out-of-sample evaluation
osres_fullmodel <- predict(full.model, testdata, type = "response")

# Use the same threshold as in-sample evaluation
ospred.fullmodel <- as.factor(ifelse(osres_fullmodel > 0.2, "1", "0"))

caret::confusionMatrix(data = ospred.fullmodel, 
                       reference = testdata$default,
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

Using the out-of-sample data, the `fullmodel` incorrectly predicted 494 actual defaults as non-default. Since this is out-of-sample data, we could say that banks might turn away 494 customers as the model predicted that they will default, although they would likely not.

### 8.2 Backward Elimination Model

```{r conMat stepmodel oos}
osres_stepmodel <- predict(bwd.model, testdata, type = "response")

ospred.stepmodel <- as.factor(ifelse(osres_stepmodel > 0.2, "1", "0"))

caret::confusionMatrix(data = ospred.stepmodel, 
                       reference = testdata$default,
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

### 8.3 LASSO Logistic Model

```{r convert testdata to matrix}
xtest <- model.matrix(default ~ ., testdata)[,-1] # Remove intercept column
ytest <- ifelse(testdata$default == "1", 1, 0)
```

```{r conMat lasso oos}
osres_lassomodel <- predict(lasso.model, xtest, type = "response")

ospred.lasso <- as.factor(ifelse(osres_lassomodel > 0.2, "1", "0"))

caret::confusionMatrix(data = ospred.lasso, 
                       reference = factor(ytest),
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

With the out-of-sample data, banks might refuse giving credit to 880 borrowers who are unlikely to default and give credit to 512 borrowers who are likely to default by using the LASSO model.

### 8.4 Summary

See table for a summary of the ranking of models in out-of-sample prediction:

| Metric      | Ranking (Out-of-Sample) |
|-------------|-------------------------|
| Accuracy    | Full < Stepwise < LASSO |
| Precision   | Full < Stepwise < LASSO |
| Recall      | LASSO < Full / Stepwise |
| Specificity | Full < Stepwise < LASSO |

Similar to the in-sample evaluation, the LASSO model did best in terms of Accuracy, Precision and Specificity scores, but worst in Recall score.

## 9 Dealing With Imbalanced Classification

To deal with imbalanced classification, we can use re-sampling techniques to under-sample a majority class, over-sample a minority class, or to use SMOTE to create synthetic data of the minority class to balance the data set. As our data set contains categorical variables, SMOTE would not be suitable as it is unable to generate synthetic categorical data. There is another technique for such data sets called SMOTE-NC for nominal and continuous variables. However, the relevant package is maintained in a github remote, and does not seem to be widely used and evaluated.

Therefore, I would use the under-sampling and over-sampling methods on the training data set, and see if it does improve prediction on the out-of-sample test data set. For simplicity and to reduce length of the project, the model I will use is the stepwise model since it has comparable, if not better, results than the full model, but is not as complicated to build as the LASSO model.

The **`ovun.sample`** function from the **`ROSE`** package can help us. 

### 9.1 Stepwise Model With Under-Sampling Method

```{r undersample}
# Set final sample size N to be twice that of the minority class, meaning it will have
# equal numbers of default and non-default observations
set.seed(123)

undersample <- ROSE::ovun.sample(formula = default ~ ., 
                                 data = traindata, 
                                 method = "under",
                                 N = nrow(traindata[traindata$default == "1",])*2)

# Check number of default and non-default observations
summary(undersample$data)
```

```{r undersample model}
un_samp.model <- step(glm(formula = default == "1" ~ .,
                          data = undersample$data, 
                          family = binomial(link = "logit")),
                      direction = "backward", 
                      k = 2,
                      trace = F)
```

### 9.2 Stepwise Model With Over-Sampling Method

```{r oversample}
# Set final sample size N to be twice that of the majority class, meaning it will have
# equal numbers of default and non-default observations
set.seed(124)

oversample <- ROSE::ovun.sample(formula = default ~ ., 
                                data = traindata, 
                                method = "over",
                                N = nrow(traindata[traindata$default == "0",])*2)

# Check number of default and non-default observations
summary(oversample$data)
```

```{r oversample model}
ov_samp.model <- step(glm(formula = default == "1" ~ .,
                          data = oversample$data, 
                          family = binomial(link = "logit")),
                      direction = "backward", 
                      k = 2,
                      trace = F)
```

## 10 Evaluation of Re-Sampling Method

I have evaluated the under-sampling and over-sampling methods on both in-sample and out-of-sample data, and compared their results to the Backward Elimination Model in Sections 7.2 and 8.2.

### 10.1 Under-Sampling Method

```{r ROC undersample is}
isres_unsamp <- predict(un_samp.model, undersample$data, type = "response")

# First argument: predicted probability, Second argument: observed response
ROCRpred <- ROCR::prediction(predictions = isres_unsamp, 
                             labels = undersample$data$default, 
                             label.ordering = c("0", "1"))

# Performance based on true positive rate and false positive rate
ROCperf <- ROCR::performance(prediction.obj = ROCRpred,
                             measure = "tpr",
                             x.measure = "fpr")

# Plot ROC and indicate threshold values, with a vertical line at 0.2
plot(ROCperf, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,1))

abline(v = 0.2)
```

```{r prec_rec undersample is}
# Performance based on precision and recall
prec_rec <- ROCR::performance(prediction.obj = ROCRpred, 
                              measure = "prec", 
                              x.measure = "rec")

# Plot Precision-Recall and indicate threshold values, with a vertical line at 0.6
plot(prec_rec, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,-0.5))

abline(v = 0.6)
```

From the under-sampling method, we can see an improvement in the ROC and Precision-Recall curves. With the same criteria of having the False Positive Rate at around 0.2 and Recall at around 0.6, the cut-off probability is now around 0.5, instead of the 0.2 used in the previous evaluations. However, I used 0.45 as the probability threshold since the trade-off is not as large for a better Recall score and TPR.

```{r conMat undersample is}
# Confusion Matrix of In-Sample Evaluation
# Use 0.45 as threshold
ispred.unsamp <- as.factor(ifelse(isres_unsamp > 0.45, "1", "0"))

caret::confusionMatrix(data = ispred.unsamp, 
                       reference = undersample$data$default, 
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

Based on in-sample evaluation, the under-sampling step-wise model has better Precision (0.7428 > 0.4611) and Recall (0.6197 > 0.5969) scores, but poorer Accuracy (0.7026 < 0.7565) and Specificity (0.7855 < 0.8018) scores. We can compare how well the model predicted the number of actual defaults since the number of actual defaults was unchanged in under-sampling. This method reduced the number of False Negatives to 2019 from 2140 in the step-wise model without under-sampling, and increased the number of True Positives to 3290 from 3169.

```{r conMat undersample oos}
# Confusion Matrix for Out-of-Sample Evaluation
osres_unsamp <- predict(un_samp.model, testdata, type = "response")

ospred.unsamp <- as.factor(ifelse(osres_unsamp > 0.45, "1", "0"))

caret::confusionMatrix(data = ospred.unsamp, 
                       reference = testdata$default,
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

However, based on the out-of-sample evaluation, the model has better Recall (0.6473 > 0.6277) score but poorer Accuracy (0.7488 < 0.7656), Precision (0.4526 < 0,4774) and Specificity (0.7776 < 0.8048) scores. It was better able to identify actual positives (859 > 833) and reduce the number of False Negatives (468 < 494). However, it was less able to identify actual negatives (3633 < 3760) and increased the number of False Positives (1039 > 912). This can be attributed to the loss of information from the majority class due to under-sampling.

### 10.2 Over-Sampling Method

```{r ROC oversample is}
isres_ovsamp <- predict(ov_samp.model, oversample$data, type = "response")

# First argument: predicted probability, Second argument: observed response
ROCRpred <- ROCR::prediction(predictions = isres_ovsamp, 
                             labels = oversample$data$default, 
                             label.ordering = c("0", "1"))

# Performance based on true positive rate and false positive rate
ROCperf <- ROCR::performance(prediction.obj = ROCRpred,
                             measure = "tpr",
                             x.measure = "fpr")

# Plot ROC and indicate threshold values, with a vertical line at 0.2
plot(ROCperf, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,1))

abline(v = 0.2)
```

```{r prec_rec oversample is}
# Performance based on precision and recall
prec_rec <- ROCR::performance(prediction.obj = ROCRpred, 
                              measure = "prec", 
                              x.measure = "rec")

# Plot Precision-Recall and indicate threshold values, with a vertical line at 0.6
plot(prec_rec, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.05),
     text.adj = c(-0.2,-0.5))

abline(v = 0.6)
```

For the over-sampling method, I would also use 0.45 as the probability threshold.

```{r conMat oversample is}
# Confusion Matrix for In-Sample Evaluation
# Use 0.45 as threshold
ispred.ovsamp <- as.factor(ifelse(isres_ovsamp > 0.45, "1", "0"))

caret::confusionMatrix(data = ispred.ovsamp, 
                       reference = oversample$data$default, 
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

Based on in-sample evaluation, the over-sampling step-wise model has better Precision (0.7380 > 0.4611) and Recall (0.6145 > 0.5969) scores, but poorer Accuracy (0.6982 < 0.7565) and Specificity (0.7818 < 0.8018) scores. 

```{r conMat oversample oos}
# Confusion Matrix for Out-of-Sample Evaluation
osres_ovsamp <- predict(ov_samp.model, testdata, type = "response")

ospred.ovsamp <- as.factor(ifelse(osres_ovsamp > 0.45, "1", "0"))

caret::confusionMatrix(data = ospred.ovsamp, 
                       reference = testdata$default,
                       dnn = c("Predicted", "Actual"),
                       positive = "1",
                       mode = "everything")
```

However, based on the out-of-sample evaluation, the model has better Recall (0.6458 > 0.6277) score but poorer Accuracy (0.7513 < 0.7656), Precision (0.4561 < 0,4774) and Specificity (0.7812 < 0.8048) scores. It was better able to identify actual positives (857 > 833) and reduce the number of False Negatives (470 < 494). However, it was less able to identify actual negatives (3650 < 3760) and increased the number of False Positives (1022 > 912).

### 10.3 Summary

We have seen how the under-sampling and over-sampling methods compare against the original step-wise model. In this sub-section, I have ranked the in-sample and out-of-sample evaluation of these two re-sampling methods against one another.

| Metric      | Ranking (In-Sample) |
|-------------|------------------|
| Accuracy    | Over < Under     |
| Precision   | Over < Under     |
| Recall      | Over < Under     |
| Specificity | Over < Under     |

| Metric      | Ranking (Out-of-Sample) |
|-------------|----------------------|
| Accuracy    | Under < Over         |
| Precision   | Under < Over         |
| Recall      | Over < Under         |
| Specificity | Under < Over         |

We can see that the under-sampling method tends to have better scores when evaluating using in-sample data, but over-sampling method works better with out-of-sample data. However, in the out-of-sample evaluation, under-sampling method had better recall scores than over-sampling method. This can probably be attributed to the replication of minority class observation in over-sampling since this would likely cause over-fitting, hence reducing detection of actual positive-class observations and/or increasing false negative predictions.

## 11 Final Remarks

Real-world data sets and models would be more complicated than this and may take into account qualitative variables as well, which would require more sophisticated algorithms. However, we can usually interpret how explanatory variables affect the response variable when using logistic regression for binary classification. We have seen how we can use step-wise models and LASSO models in logistic regression for variable selection, and how re-sampling can be used to tackle imbalanced data sets. It would be interesting to find out how different proportions of majority and minority class would lead to better model evaluations than using a 50-50 balanced data set.

It is important to note that the models in this project do not consider a cost to misclassification, which is an important factor in real-world problems. There could also be a trade-off involved in the costs of false positives and false negatives and there would be a need to weigh these costs to select the best model to be implemented.

## References

<https://bookdown.org/tpinto_home/Regularisation/lasso-regression.html>

<https://journalofbigdata.springeropen.com/articles/10.1186/s40537-018-0143-6>

<https://quantifyinghealth.com/stepwise-selection/>

<https://www.mygreatlearning.com/blog/understanding-of-lasso-regression/>

<http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/4-5-Multiple-collinearity.html>
