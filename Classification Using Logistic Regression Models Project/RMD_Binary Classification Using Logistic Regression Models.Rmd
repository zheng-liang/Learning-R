---
title: "Binary Classification Using Logistic Regression Models"
output: 
  github_document:
    toc: true
---

# 1 Introduction

The purpose of this project was to apply what I had learnt about logistic regression and classification in machine learning (ML). It documents (1) the packages and functions that are useful for logistic classification and ML, and (2) the steps to building a logistic classification model in R.

The project has three main parts. In the first part, I introduced what logistic regression is and why it is used in classification in machine learning. In the second part, I provided examples of logistic regression in binary classification (i.e. there are only two outcomes) using an in-built dataset in R. In the last part, I applied the logistic regression along with more practical approaches to variable selection on a separate dataset.

This project focused on how to use logistic regression models in binary classification, and less about the mathematical application and theory behind it as these can be found on various websites and textbooks.

## 1.1 What is logistic regression and why is it used in classification problems?

Logistic regression is commonly used to determine the relationship between binary outcomes and one or more independent variables. The logistic regression model is written as

$$
\log\bigg(\frac{p(y=1)}{1-p(y=1)}\bigg) = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n
$$
and shows a linear relationship between the independent variables and the log-odds (or logit), where the odds refers to $\frac{p}{1-p}$. The independent variables can be categorical or continuous variables. The dependent variable has only two outcomes (in general), for example yes or no, male or female, default or no default.

The coefficients of the independent variables can be interpreted as the effect of an increase in $x_n$ on the log-odds. The probability where $y=1$can also be obtained through some mathematical operations, though this can be done automatically in R. By setting a cut-off probability or threshold value, we can classify whether an observation is likely to fall under $y=1$ or $y=0$. 

Logistic regression can be simple to understand and interpret, although it may not be the best classification model in comparison to more sophisticated ML algorithms.

## 1.2 What are the assumptions needed for logistic regression?

This sub-section lists the common assumptions of logistic regressions and some of the ways to check them.

### 1.2(a) Type of logistic regression matches the response variable

The type of logit regression needs to match the type of outcome. In most cases, logit regression is used for binary outcomes but there are cases where it might be used for multinomial or ordinal outcomes and appropriate models need to be used for each type.

**To check assumption**: Check the number of unique outcomes/response variable. If there are only two outcomes, it is very likely to be a binary classification problem. If there are more than two outcomes, we are dealing with multinomial or ordinal classification problems.

### 1.2(b) Observations are independent

Observations need to be independent of one another, and not be affected by other observations.

**To check assumption**: Plot the residuals against order of observations and check if there is a random pattern. If pattern is not random, it indicates possibility of correlation between observations. However, autocorrelation should be less of a concern with cross-sectional data as long as the design of the study ensures that data are collected from random samples and there are no repeated or paired observations.

### 1.2(c) No multicollinearity among explanatory variables

Independent variables should not be highly correlated with one another as it affects the variance of estimated coefficients and statistical significance of estimated coefficients becomes unreliable.

**To check assumption**: A simple method would be to use a correlation matrix to find highly correlated explanatory variables. Another method is to calculate the variance inflation factor (VIF) of each explanatory variable. A common rule of thumb is that VIF more than 10 indicates problem of multicollinearity.

### 1.2(d) Linear relationship between continuous explanatory variables and logit of response variable

Continuous explanatory variables and the logit of the response variable (or log-odds) have a linear relationship. Logit regression does not require linearity between continuous explanatory variables and response variable.

**To check assumption**: Plot log-odds against each continuous explanatory variable and check for linear relationship

### 1.2(e) No highly influential outliers

There should be no influential outliers that could affect the outcome of the model. Removing such observations are possible if they are determined to be incorrectly entered or measured. It is also possible to compare the regression results with and without the outliers, and note how the results differ.

**To check assumption**: Calculate Cook's Distance to find influential observations and standardized residuals to find outliers.

### 1.2(f) Large sample size

A large sample size is needed to produce conclusive results.

# 2 Packages Required

```{r load packages, message=FALSE}
# Use install.packages("packagename") if the packages are not installed in R
library(car) 
library(caret) # For building ML models
library(corrplot) # For correlation plot
library(tidyverse) # For ggplot and dplyr packages
library(mlbench) # For data sets
library(lmtest) # For statistical tests
library(glmnet) # For LASSO logistic regression
```

These packages provide the necessary functions and data sets that can be used for learning and building logistic classification models.

# 3 Basics of Logistic Regression

This section covers how to perform logistic regression and solve binary classification problems using an in-built dataset.

I selected the Pima Indians Diabetes Database in the **`mlbench`** package for this example. The purpose is to determine the probability of a person having diabetes based on several variables such as the person's body mass index, age, and blood pressure.

## 3.1 Exploratory Data Analysis

Let us first load the data and perform an Exploratory Data Analysis before the modelling task.

```{r load PimaIndiansDiabetes dataset}
data("PimaIndiansDiabetes")
```

```{r structure and summary}
str(PimaIndiansDiabetes)

summary(PimaIndiansDiabetes)
```

```{r head and tail of df}
head(PimaIndiansDiabetes, n = 4)

tail(PimaIndiansDiabetes, n = 4)
```

By entering `??PimaIndiansDiabetes` into the console, we can get a description of the variables in the data frame. I have listed the variables and its description in the table below.

| Variable | Description                                           |
|----------|-------------------------------------------------------|
| pregnant | Number of times pregnant                              |
| glucose  | Plasma glucose concentration (glucose tolerance test) |
| pressure | Diastolic blood pressure (mm Hg)                      |
| triceps  | Triceps skin fold thickness (mm)                      |
| insulin  | 2-Hour serum insulin (mu U/ml)                        |
| mass     | Body mass index (weight in kg/(height in m)^2)        |
| pedigree | Diabetes pedigree function                            |
| age      | Age (years)                                           |
| diabetes | Class variable (test for diabetes)                    |

The **`str`** function shows that there are 768 observations with 9 variables and only the variable *`diabetes`* is a factor while all other variables are numeric.

The **`summary`** function reveals that there are possibly some errors in the data, with value of zero in variables *`glucose`*, *`pressure`*, *`triceps`*, and *`mass`*. I would remove these observations to keep things simple. The imputation of these variables would require methods beyond using the mean or median of each variable.

```{r filter out obs}
data <- PimaIndiansDiabetes %>%
  dplyr::filter(glucose != 0 & pressure != 0 & triceps != 0 & mass != 0)

summary(data)
```

It seems that *`insulin`* has values of zero as well, which could be due to not receiving insulin despite having diabetes. Removing these observations might impact the results and analysis.

Now that there is a better understanding of the variables in the dataset, I would plot these variables to visualize their distribution.

```{r split response and explvar}
response <- subset(data, select = c(diabetes))

explvar <- subset(data, select = -c(diabetes))

summary(response)
summary(explvar)
```

Let us take a look at the distribution of the response and explvar variables in a univariate plot. This gives us a visual understanding of how the variables are distributed individually.

```{r dist of diabetic test, fig.width=6, fig.height=4.5, fig.align='center'}
ggplot(data = response) + 
  geom_bar(aes(x = diabetes, fill = diabetes)) + 
  ggtitle("Distribution of Outcomes from Diabetes Test") +
  xlab("Diabetes") + ylab("Frequency") +
  theme(plot.title = element_text(hjust = 0.5))
```

We have about twice as many negative observations compared to positive observations. While this may not be a problem in our case, it would be good to think about the representation of observations in a more complex dataset. Under-representation of any group in an analysis may lead to unreliable results and analysis.

```{r boxplot of dependent variables, fig.width=6, fig.height=4.5, fig.align='center'}
par(mfrow = c(1, 4), mar = c(1, 3, 2, 1))

for (i in 1:4) {
  boxplot(explvar[,i], main = names(explvar)[i])
}

for (i in 5:8) {
  boxplot(explvar[,i], main = names(explvar)[i])
}
```

Plotting the explanatory variables using histograms allow us to visually check how the data is distributed. The plots show that most variables do not have a normal distribution, which we can further check with the Shapiro-Wilk's test.

```{r histogram of dependent variables, fig.width=6, fig.height=4.5, fig.align='center'}
par(mfrow = c(1, 4), mar = c(2, 3, 2, 1))

for (i in 1:4) {
  hist(explvar[,i], main = names(explvar)[i])
}

for (i in 5:8) {
  hist(explvar[,i], main = names(explvar)[i])
}
```

```{r shapirotest}
store_test <- list()

for (i in 1:8) {
  store_test <- cbind(store_test, 
                      shapiro.test(x = explvar[,i]))
}

store_test["data.name",] <- names(explvar)

store_test

store_test["p.value",] > 0.05
```

We can use multivariate plots to visualize how variables interact with one another. Using a correlation matrix heatmap, we can visualize the correlation between variables and check for the presence of multicollinearity between explanatory variables. Since the Shapiro-Wilk's test rejected the hypothesis of normal distribution for the explanatory variables, I would use the Spearman correlation instead of Pearson correlation.

```{r correlation matrix heatmap, fig.align='center'}
corrX <- cor(x = explvar, method = "spearman")

corrplot(corr = corrX, 
         method = "color", 
         addCoef.col = "black",
         title = "Correlation Between Explanatory Variables",
         mar = c(0, 0, 1, 0))
```

From the correlation matrix, there does not seem to be any highly correlated explanatory variables, which may satisfy our assumption that there is no multicollinearity.

Now that we have a better understanding of the variables in the dataset, we can proceed with the modelling process.

## 3.2 Building the Binary Logistic Regression Model

To start off, we can create a logistic regression model that uses all the explanatory variables. We need to use the **`glm`** function and for the **`family`** argument, we indicate `binomial(link = "logit")` for logistic regression.

```{r create logit model1}
# Full model
model1 <- glm(diabetes == "pos" ~ ., #indicate "pos" so that it is 1 and "neg" is 0
              data = data, 
              family = binomial(link = "logit"))

summary(model1)
```

The coefficients (or "Estimate" in the regression output) of the explanatory variables refers to the effect on the log-odds and not the probability itself.

Variables *`pressure`*, *`triceps`*, *`insulin`*, and *`age`* has p-values more than the significance level of 5%. We may consider removing them from the equation and see if we get better results. The Likelihood Ratio test will be needed to test if the difference is significant, which is something like an F-test for Ordinary Least Squares regression models. The null hypothesis is that the full model and the restricted model fits the data equally well and we should use the restricted model. The alternative is to use the full model.

```{r create logit model2}
# Restricted model
model2 <- glm(diabetes == "pos" ~ pregnant + glucose + mass + pedigree, 
              data = data,
              family = binomial(link = "logit"))

summary(model2)
```

```{r lrtest1}
lmtest::lrtest(model2, model1)
```

Since the **`lrtest`** function returned a p-value more than 5%, we do not reject the null and hence, we will use the restricted model for our analysis.

### 3.2(a) Testing Multicollinearity Assumption

In Section 3.1 Exploratory Data Analysis, I used the correlation matrix to determine if there are any highly correlated explanatory variables. Another method is to use the Variance Inflation Factor.

```{r vif1}
car::vif(model2)
```

None of the variables has a VIF of more than 10. Therefore, it is likely that there is no multicollinearity among the explanatory variables.

### 3.2(b) Testing Linearity Assumption

To check if there is a linear relationship between the continuous explanatory variables and the logit of the response variable, we need to obtain the fitted probabilities from our model. The **`fitted.values`** function returns the probability and not the log-odds.

```{r fitted values from model2}
prob.model2 <- fitted.values(model2)

head(prob.model2)
```

```{r calculate logit and bind into data}
pred <- names(coef(model2)[-1]) # Exclude intercept

plotdata <- subset(data, select = pred) # Only keep the variables used in the model

plotdata <- plotdata %>%
  mutate(logit = log(prob.model2/(1-prob.model2))) %>% # Bind logit to plotdata
  gather(key = "pred", value = "pred.value", -logit) 
```

```{r plot variables against logit, message=FALSE, fig.align='center'}
ggplot(data = plotdata, aes(x = logit, y = pred.value)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "loess") +
  facet_wrap(. ~ pred, scales = "free")
```

We can see that variables *`glucose`*, *`mass`* and *`pregnant`* are have a rather linear relationship, but *`pedigree`* has a non-linear relationship with the log-odds. I have attempted to transform the variable using the natural log.

```{r create logit model3}
model3 <- glm(diabetes == "pos" ~ pregnant + glucose + mass + log(pedigree),
              data = data,
              family = binomial(link = "logit"))

summary(model3)
```

```{r store fitted values from model3 and calculate logit}
prob.model3 <- fitted.values(model3)

logit = log(prob.model3/(1-prob.model3))
```

```{r transfrom pedigree with log}
plotdata2 <- subset(data, select = pred)

plotdata2 <- plotdata2 %>%
  mutate(l.pedigree = log(pedigree))
```

```{r plot logped against logit, fig.width=6, fig.height=4.5, fig.align='center'}
ggplot(data = plotdata2, aes(x = logit, y = l.pedigree)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "loess")
```

After the log transformation, *`pedigree`* has a more linear relationship with the log-odds compared to before.

### 3.2(c) Checking for Highly Influential Outliers

This requires that we find outliers that are highly influential. To determine the observations that have the highest influence, we can plot the Cook's Distance generated from the model or calculate them using **`cooks.distance`** function. To determine the outliers, we use the standardized residuals and check if there are any observations with absolute values greater than 3. We can also use the **`outlierTest`** function in the **`car`** package to calculate the studentized residuals.

```{r cooks distance}
plot(model3, which = 4)

head(sort(cooks.distance(model3), decreasing = T))
```

```{r std resid}
std.resid <- rstandard(model3)

val <- abs(std.resid) > 3

table(val)["TRUE"] # Returned NA because there are no TRUE logical values
```

```{r studentized resid1}
car::outlierTest(model3)
```

I had found no outliers that were highly influential based on the tests that were conducted. If outliers were found, instead of removing them, it is better to investigate these observations to check if they were erroneously measured or recorded.

## 3.3 Evaluating the Models

Now that we have created a few models, we can evaluate how each model performs in predicting the in-sample observations. We first store the fitted values of different models and I will set the cutoff at 0.5 to determine whether an observation would be positive or negative to make things simple for now.

### Model 1: Full Model

```{r prob model1}
prob.model1 <- fitted.values(model1) #can also use predict(model1, type="response")
```

```{r prediction and conMat model1}
prediction.model1 <- as.factor(ifelse(prob.model1 > 0.5, "pos", "neg"))

confusionMatrix(data$diabetes,
                prediction.model1,
                dnn = c("Actual", "Predicted"))
```

`model1` was able to predict 316 negatives that are actually negatives and predict 101 positives that are actually positive. It predicted 76 negatives when they are actually positive observations and predicted 39 positive observations when they are actually negative. Since this is a model to predict whether a person is diabetic, the 76 false negative would be a problem as it would result in life-threatening situations. The 39 false positive could be lowered, but there is always a trade-off in using prediction models and we should also consider the costs involved in real-life situations.

### Model 2: Restricted Model (Not Transformed)

```{r prediction and conMat model2}
prediction.model2 <- as.factor(ifelse(prob.model2 > 0.5, "pos", "neg"))

confusionMatrix(data$diabetes,
                prediction.model2,
                dnn = c("Actual", "Predicted"))
```

### Model 3: Restricted Model with Transformation

```{r prediction and conMat model3}
prediction.model3 <- as.factor(ifelse(prob.model3 > 0.5, "pos", "neg"))

confusionMatrix(data$diabetes, 
                prediction.model3,
                dnn = c("Actual", "Predicted"))
```

We can see that Models 2 and 3 have the same evaluation but these models did better than Model 1.

# 4 Binary Classification Using Other Dataset With Out-of-Sample Evaluation

In Section 3, I explored the basics of logistic regression using the **`glm`** function on the `Pima Indians Diabetes` dataset. However, real world datasets are likely to have much more attributes/variables which we might not be able to select variables simply based off p-values to indicate significance of variables. Another possible problem was the usefulness of the model to predict outcomes on out-of-sample data. Our cutoff value to determine whether a person is positive or negative was also rather arbitrary, and we could improve the prediction by giving more thought into this cutoff value. 

I sought to address these problems in this section by applying more real-world techniques and comparing the predictive power of the different models. However, one should remember that the purpose of machine learning and statistical modelling are different. Machine learning usually aims for best prediction while statistical modelling might be more towards the inference of explanatory variables to the response variable.

## 4.1 Dataset and Exploratory Data Analysis

The dataset that I used is called [`default of credit card clients`](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients), which can be found in the UCI Machine Learning Repository. It contains customer credit default data from Taiwan in 2005. Other than the binary classification of default and non-default, the probability of default may also be an important result. The file downloaded is an Excel file and to load it into R, we need the **`readxl`** package.

```{r load readxl package}
library(readxl)
```

```{r load dataset from wd, message=FALSE}
# This step loads the file from the working directory. If unsure of wd, use 
# getwd() to check the location of the wd and setwd() to set the wd where the file is saved.

default.data <- data.frame(read_excel("credit card default.xls"))
```

Let us take a peek at `default.data`.

```{r head tail}
head(default.data)

tail(default.data)

colnames(default.data)
```

From the **`head`** function, we saw that the first row should be the column names of the data frame and the first column should be the row names. There are many variables, 24 to be exact, with 1 dependent variable (Y, the last column) and 23 independent variables (X1 to X23). I would start by adjusting the column and row names.

```{r adjust column names}
colnames(default.data) <- default.data[1,]

default.data <- default.data[-1,]

head(default.data)
```

```{r adjust row names}
# Remove the first column since ID matches the auto-assigned row numbers.
default.data <- default.data[,-1]

rownames(default.data) <- NULL # Reset the row numbers

head(default.data)
```

Next, I would change the some of the column names to be something more descriptive but it is optional.

```{r renaming columns}
newnames <- c("PAY_SEP","PAY_AUG","PAY_JUL", "PAY_JUN", "PAY_MAY", "PAY_APR",
              "BILLAMT_SEP","BILLAMT_AUG","BILLAMT_JUL", "BILLAMT_JUN", "BILLAMT_MAY", "BILLAMT_APR",
              "PAYAMT_SEP","PAYAMT_AUG","PAYAMT_JUL", "PAYAMT_JUN", "PAYAMT_MAY", "PAYAMT_APR")

oldnames <-colnames(default.data %>%
                      dplyr::select(grep("PAY" , names(default.data)), 
                                    grep("BILL", names(default.data)))) 

colnames(default.data)[colnames(default.data) %in% oldnames] <- newnames

default.data <- default.data %>%
  rename(default = `default payment next month`)

colnames(default.data)
```

I have listed the description of the variables in the table below, which can be found in the UCI Machine Learning Repository where the dataset was downloaded from.

| Variable                  | Description                                                                                                         |
|------------------------|------------------------------------------------|
| LIMIT_BAL                 | Amount of the given credit, including both the individual consumer credit and his/her family (supplementary) credit |
| SEX                       | Gender. Male = 1, Female = 2                                                               |
| EDUCATION                 | Graduate School = 1, University = 2, High School = 3, Others = 4                                                    |
| MARRIAGE                  | Married = 1, Single = 2, Others = 3                                                                                    |
| AGE                       | In years                                                                                      |
| PAY_SEP - PAY_APR         | History of past payment from April to September 2005. Pay duly = -1, Delay one month = 1, Delay two months = 2, ..., Delay nine months or more = 9                        |
| BILLAMT_SEP - BILLAMT_APR | Amount of bill statement from April to September 2005.                                                                                          |
| PAYAMT_SEP - PAYAMT_APR   | Amount of previous payment.                                                                                                          |
| default                   | Response variable. Yes = 1, No = 0.                                                                                 |

Now, we can begin exploring the data set through an Exploratory Data Analysis.

```{r structure}
str(default.data)
```

We can see that all the data values has been loaded as characters, and we need to change this to their appropriate types (numeric, factors, etc.).

```{r changing value types}
default.data <- default.data %>% 
  mutate(
    across(
      c(grep("PAYAMT", names(default.data)), 
        grep("BILLAMT", names(default.data)), 
        "AGE", "LIMIT_BAL"), 
      as.numeric))

default.data <- default.data %>%
  mutate(
    across(
      c("SEX", "EDUCATION", "MARRIAGE", "default", 
             grep("PAY_", names(default.data))), 
      as.factor))

str(default.data)
```

```{r summary}
summary(default.data)
```

We can see that there are some data that stands out. For example, in the variables *`EDUCATION`* and *`MARRIAGE`*, there is a value of 0 which represents none of the known classes in each of the variables. The variables *`PAY_`* has a value of -2, which was also not indicated in the description of the data. It is likely to be prepayments as variables *`BILLAMT_`* has negative values. 

Let us clean the data for variables *`EDUCATION`* and *`MARRIAGE`*. Since they have each have a class called `Others`, we can consider replacing the unknown classes with this class.

```{r replace unknown class with others}
# For EDUCATION
levels(default.data$EDUCATION)[levels(default.data$EDUCATION) %in% c("0", "5", "6")] <- "4"

# For MARRIAGE
levels(default.data$MARRIAGE)[levels(default.data$MARRIAGE) == "0"] <- "3"

summary(default.data)
```

We can plot out the variables in a univariate and multivariate plots to visualize how they are distributed and correlate with one another.

### 4.1.1 Univariate Plots

Using the package **`gridExtra`** allows us to arrange a number of plots into a single page.

```{r load gridExtra package, message=FALSE}
library(gridExtra)
```

```{r dist of sex_educ_marr_age, message=FALSE, fig.align="center"}
# For categorical and discrete variables
sex <- ggplot(data = default.data) +
  geom_bar(aes(x = SEX, fill = default)) +
  theme(legend.title = element_text(size = 10),
        legend.key.size = unit(0.5, "cm"),
        axis.title.x = element_text(size = 10))

education <- ggplot(data = default.data) + 
  geom_bar(aes(x = EDUCATION, fill = default)) +
  theme(legend.title = element_text(size = 10),
        legend.key.size = unit(0.5, "cm"),
        axis.title.x = element_text(size = 10))

marriage <- ggplot(data = default.data) +
  geom_bar(aes(x = MARRIAGE, fill = default)) +
  theme(legend.title = element_text(size = 10),
        legend.key.size = unit(0.5, "cm"),
        axis.title.x = element_text(size = 10))

age <- ggplot(data = default.data) +
  geom_histogram(aes(x = AGE, fill = default)) +
  theme(legend.title = element_text(size = 10),
        legend.key.size = unit(0.5, "cm"),
        axis.title.x = element_text(size = 10))

gridExtra::grid.arrange(sex, education, marriage, age, ncol = 2, nrow = 2)
```

From the plots, it is hard to determine if the *`default`* variable is correlated with gender, education, marriage and age. This is because while we see higher default in certain groups, those groups also had more observations which allows us to find more people who defaulted.

```{r dist billamt and payamt, fig.align="center"}
# For continuous variables
billamt <- default.data %>%
  select(grep("BILLAMT", names(default.data)))

payamt <- default.data %>%
  select(grep("PAYAMT", names(default.data)))

par(mfrow = c(1, 6), mar = c(1, 3, 3, 2))
for (i in 1:6) {
  boxplot(billamt[,i], main = names(billamt)[i])
}

par(mfrow = c(1, 6), mar = c(1, 3, 3, 2))
for (i in 1:6) {
  boxplot(payamt[,i], main = names(payamt)[i])
}
```

From the boxplots, we can tell that the variables are not normally distributed as they have a very long tail (a lot of outlier points).

### 4.1.2 Multivariate Plots

Let us visualize the correlation between the numeric variables in `default.data`.

```{r corrplot, fig.width=6, fig.height=4.5, fig.align="center"}
correl <- default.data %>% 
  select(where(is.numeric)) %>%
  cor(method = "spearman")

corrplot(corr = correl,
         method = "color",
         type = "lower")
```

We can see that the correlation between *`BILLAMT_`* variables is very high, which is likely due to the time component involved in these variables. We could expect that if the bill statement in April is high, then the bill statement in May would likely be high as well. However, it seemed that the correlation between the *`PAYAMT_`* is rather moderate. For the correlation between *`BILLAMT_`* and *`PAYAMT_`*, there seems to be a pattern of relatively higher correlation on the diagonals, i.e. between *`BILLAMT_AUG`* and *`PAYAMT_SEP`* or *`BILLAMT_JUL`* and *`PAYAMT_AUG`*. Due to the high correlation between *`BILLAMT_`* variables, I may need to create new features to reduce the multicollinearity.

### 4.1.3 Creating New Variables

The variable I created is the average of each observation's *`BILLAMT_`* variables, divided by its *`LIMITT_BAL`*. This new variable would help us determine the average utilization of the credit given to a person.

```{r avg util}
avg_util <- round((rowMeans(billamt) / default.data$LIMIT_BAL)*100, 4)
```

Another variable I created is the average payment over the period April to September.

```{r avg pmt}
avg_pmt <- round(rowMeans(payamt), 4)
```

I have bound the variables of interest into a new data frame called `new_data`.

```{r bind variables}
new_data <- data.frame(default.data[,1:11],
                       avg_util,
                       avg_pmt,
                       default = default.data$default)

head(new_data, n = 4)
```

The variables *`PAY_`* has too many classes, and there would be a need to reduce them. Looking at the variables *`PAY_`* in a table format, I have reduced the original 11 classes into 2. 0 = those who paid duly/early or delayed payment for one month, 1 = those who delayed payment for two month or more.

```{r contingent table}
cont_tab <- list(sep = table(new_data$default, new_data$PAY_SEP),
                 aug = table(new_data$default, new_data$PAY_AUG),
                 jul = table(new_data$default, new_data$PAY_JUL),
                 jun = table(new_data$default, new_data$PAY_JUN),
                 may = table(new_data$default, new_data$PAY_MAY),
                 apr = table(new_data$default, new_data$PAY_APR))

cont_tab
```

```{r changing classes}
for (i in 6:11) {
  levels(new_data[,i])[levels(new_data[,i]) %in% c("-2", "-1", "0", "1")] <- "0"
  
  levels(new_data[,i])[!levels(new_data[,i]) %in% c("-2", "-1", "0", "1")] <- "1"
}

summary(new_data)

str(new_data)
```

I calculated the correlation and plotted the correlation matrix to see if the numeric variables in `new_data` data frame are highly correlated. It can be seen that none of the variables have a correlation of more than 0.7, so multicollinearity might not be a problem.

```{r correl new var, fig.width=6, fig.height=4.5, fig.align="center"}
correl <- new_data %>% 
  select(where(is.numeric)) %>%
  cor(method = "spearman")

corrplot(corr = correl,
         method = "color",
         type = "lower",
         addCoef.col = "black")
```

## 4.2 Building the Models

Now that I have settled on the variables to be used for the logistic regression model, I can start to built the different models that are commonly used. Before modelling, I would split the data into training and testing sets.

```{r split data}
# Split data into 80% training and 20% testing sets
set.seed(1)
validation <- createDataPartition(new_data$default, p = 0.80, list = F)

traindata <- new_data[validation,]

testdata <- new_data[-validation,]
```

### 4.2.1 Full Model

I would start with building a full model that contains all the explanatory variables.

```{r full model}
full.model <- glm(default == "1" ~ .,    # 1 refers to those who default
                  data = traindata,
                  family = binomial(link = "logit"))

summary(full.model)
```

I used the model to calculate the variance inflation factor of each variable to test for multicollinearity. It can be seen that none of the variables has a large VIF value, so multicollinearity may not be a problem.

```{r vif2}
car::vif(full.model)
```

### 4.2.2 Restricted Model

From the regression results of the `full.model`, there are the *`MARRIAGE`* variable seemed to be statistically insignificant, so I removed them while keeping the rest of the variables in the restricted model.

```{r rest model wo marriage}
# Remove marriage variables
rest.model <- glm(default == "1" ~ .-MARRIAGE,
                   data = traindata,
                   family = binomial(link = "logit"))

summary(rest.model)
```

I did a likelihood ratio test and found that the full model is preferred since the p-values showed strong evidence against the null hypothesis.

```{r lrtest2}
lmtest::lrtest(full.model, rest.model) # Result: Use full model
```

### 4.2.3 Checking for Influential Outliers in the Full Model

I checked for highly influential observations using Cook's Distance and outliers using standardized and studentized residuals.

```{r cooksd}
# Influence using Cook's Distance
plot(full.model, which = 4)

head(sort(cooks.distance(full.model), decreasing = T))
```

```{r standardized resid}
# Outliers with standardized/studentized residuals
std.resid <- rstandard(full.model)

val <- abs(std.resid) > 3

table(val)

val[val == "TRUE"] # Can be used to further investigate the observations
```

```{r studentized resid2}
car::outlierTest(full.model) # No outliers detected by studentized residuals
```

There seemed to be 1 influential outlier detected by the standardized residuals, which could be further investigated if necessary.

### 4.2.4 Checking Linearity Assumption of Full Model

To check the linearity assumption, I first found the fitted values (or the probability) using the full model. Next, I calculated the log-odds (or logit), and finally plotted the log-odds against all continuous variables in my model.

```{r fitvalues and logit}
prob.fullmod <- fitted.values(full.model)

logit = log(prob.fullmod/(1-prob.fullmod))
```

```{r plot, fig.align="center"}
p1 <- ggplot(data = traindata, aes(x = logit, y = LIMIT_BAL)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

p2 <- ggplot(data = traindata, aes(x = logit, y = AGE)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

p3 <- ggplot(data = traindata, aes(x = logit, y = avg_util)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

p4 <- ggplot(data = traindata, aes(x = logit, y = avg_pmt)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess")

gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)
```

It would seem that the continuous variables are mostly linear to the log-odds, maybe other than for the *`LIMIT_BAL`* variable.

### 4.2.5 Evaluating the Performance of the Full Model

To evaluate how well the full model is able to predict whether a person would default, we need to determine the threshold value. The value will be determined using the Receiver Operating Curve (ROC) and I tested the performance on in-sample and out-of-sample data.

```{r prob using full model}
# Returns the probability of default
res_fullmodel <- predict(full.model, traindata, type = "response")
```

```{r ROC and plot}
# First argument: predicted probability, Second argument: observed response
ROCpred <- ROCR::prediction(res_fullmodel, traindata$default)

# Performance based on true positive rate and false positive rate
ROCperf <- ROCR::performance(ROCpred, "tpr", "fpr")

# Plot ROC and indicate threshold values
plot(ROCperf, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.01),
     text.adj = c(-0.2,1))
```

```{r conMat fullmodel is}
# Use 0.17 as threshold
pred.fullmodel <- as.factor(ifelse(res_fullmodel > 0.17, "1", "0"))

caret::confusionMatrix(traindata$default, 
                       pred.fullmodel, 
                       dnn = c("Actual", "Predicted"))
```

Using `fullmodel` on the in-sample data, 12746 observations were correctly predicted as non-default and 3638 were correctly predicted as default. Since non-default is the positive class, there were 5946 false negative and 1671 false positive. In other words, the model incorrectly predicted 1671 actual defaults as non-defaults. This could have serious implications to a financial institution depending on the size of the credit. The 5946 actual non-defaults predicted as defaults would also have implications in the form of higher monitoring costs, or financial institutions might reject giving credit to borrowers who are unlikely to default.

```{r conMat fullmodel oos}
# Change to testdata instead for out-of-sample evaluation
res_fullmodel <- predict(full.model, testdata, type = "response")

# Use the same threshold as in-sample evaluation
pred.fullmodel <- as.factor(ifelse(res_fullmodel > 0.17, "1", "0"))

caret::confusionMatrix(testdata$default,
                       pred.fullmodel,
                       dnn = c("Actual", "Predicted"))
```

Using the out-of-sample data, the `fullmodel` incorrectly predicted 385 actual defaults as non-default. However, in terms of the accuracy of the model, it was the same as using in-sample data. Since this is out-of-sample data, we could say that banks might turn away 1514 customers as the model predicted that they will default, although they would not.

## 4.3 Stepwise Model Using Backward Elimination

These techniques selects variables that are usually statistically significant or meet certain requirements, such as lowest AIC or BIC. However, there are many arguments online and from research articles against using such techniques. However, it does provide a quick and easy way to get a model from multiple explanatory variables and we can use it as a benchmark.

### 4.3.1 Creating the Model

Backward elimination starts off with a model that contains all the explanatory variables and removes them one at a time to reduce AIC, with the variable that causes the largest reduction to be removed first. Before that, let us create a null model that does not use any explanatory variables.

```{r null model}
# Create a null model that predicts using grand mean
null.model <- glm(default == "1" ~ 1,
                  data = traindata,
                  family = binomial(link = "logit"))

summary(null.model)
```

```{r bwd model}
bwd.model <- step(full.model,
                  scope = list(lower = null.model, upper = full.model),
                  direction = "backward", 
                  k = 2,
                  trace = T)
# Indicate scope argument to define range of models to examine
# Indicate direction = "backward" for backward elimination
# Indicate k = 2 for AIC, k = log(n) for BIC
# Indicate trace = F if want to suppress the running of the step by step regression

summary(bwd.model)
```

Based on the backward elimination approach, the full model (containing all explanatory variables) leads to the lowest AIC.

### 4.3.2 Evaluating the Stepwise Model

Since the stepwise model using backward elimination is the same as the full model is Section 4.2, the accuracy of the model would be the same.

```{r ROC stepmodel is}
res_stepmodel <- predict(bwd.model, traindata, type = "response")

ROCpred <- ROCR::prediction(res_stepmodel, traindata$default)

ROCperf <- ROCR::performance(ROCpred, "tpr", "fpr")

plot(ROCperf, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.01),
     text.adj = c(-0.2,1))
```

```{r conMat stepmodel is}
# Use 0.17
pred.stepmodel <- as.factor(ifelse(res_stepmodel > 0.17, "1", "0"))

caret::confusionMatrix(traindata$default,
                       pred.stepmodel,
                       dnn = c("Actual", "Predicted"))
```

```{r conMat stepmodel oos}
res_stepmodel <- predict(bwd.model, testdata, type = "response")

pred.stepmodel <- as.factor(ifelse(res_stepmodel > 0.17, "1", "0"))

caret::confusionMatrix(testdata$default,
                       pred.stepmodel,
                       dnn = c("Actual", "Predicted"))
```

## 4.4 LASSO Logistic Model

Another method of selecting explanatory variables is the LASSO regression, which shrinks the coefficients of variables that are not important to zero. This can be done in R using the **`glmnet`** package.

### 4.4.1 Converting Data Frame to Matrix

Before we can run the lasso logistic regression, the data frame has to be converted into a matrix for **`glmnet`** to run. We also need to convert categorical variables into suitable dummy variables. All this can be done using the **`model.matrix`** function.

```{r model.matrix}
x_var <- model.matrix(default ~ ., traindata)[,-1] # Remove intercept column

y_var <- ifelse(traindata$default == "1", 1, 0)
```

### 4.4.2 Calculating lambda

For LASSO regression, there is a need to specify the lambda value, which is the amount of the penalty added to the log-likelihood function. The optimal lambda value can be obtained using the **`cv.glmnet`** function. I have chosen to use the `lambda.1se` value which would give a simpler model, but `lambda.min` can also be chosen to obtain a more complex model.

```{r lambda}
lasso.lambda <- glmnet::cv.glmnet(x_var, 
                                  y_var, 
                                  alpha = 1,
                                  family = "binomial",
                                  type.measure = "class") # lambda that optimizes the misclassification error
# alpha = 1 for lasso, family = "binomial" for logistic regression
```

```{r optimal lambda}
# optimal value of lambda
opt.lambda <- lasso.lambda$lambda.1se

opt.lambda
```

### 4.4.3 Fitting the LASSO Logistic Model

Now that the lambda value has been obtained and stored, the LASSO logit regression model can be fitted. Unfortunately, the **`summary`** function does not show the coefficients like the models fitted using **`glm`**. To extract the intercept and coefficients, add `$a0` and `$beta` to the model name.

```{r lasso model}
lasso.model <- glmnet(x_var,
                      y_var,
                      alpha = 1,
                      family = "binomial",
                      lambda = opt.lambda)

lasso.model$a0; lasso.model$beta # intercept and coefficients
```

### 4.4.4 Evaluating the LASSO Logistic Model

```{r ROC lasso is}
res_lassomodel <- predict(lasso.model, x_var, type = "response")

ROCpred <- ROCR::prediction(res_lassomodel, factor(y_var))

ROCperf <- ROCR::performance(ROCpred, "tpr", "fpr")

plot(ROCperf, 
     colorize = T, 
     print.cutoffs.at = seq(0, by = 0.01),
     text.adj = c(-0.2,1))
```

```{r conMat lasso is}
## Use 0.17 as threshold
pred.lasso <- as.factor(ifelse(res_lassomodel > 0.17, "1", "0"))

caret::confusionMatrix(factor(y_var),
                       pred.lasso,
                       dnn = c("Actual", "Predicted"))
```

The LASSO logistic regression model has led to a higher accuracy than the full model and the stepwise model. However, there is an increase in the misclassification of actual defaults as non-defaults.

```{r convert testdata to matrix}
xtest <- model.matrix(default ~ ., testdata)[,-1] # Remove intercept column
ytest <- ifelse(testdata$default == "1", 1, 0)
```

```{r conMat lasso oos}
res_lassomodel <- predict(lasso.model, xtest, type = "response")

pred.lasso <- as.factor(ifelse(res_lassomodel > 0.17, "1", "0"))

caret::confusionMatrix(factor(ytest),
                       pred.lasso,
                       dnn = c("Actual", "Predicted"))
```

With the out-of-sample data, banks might refuse giving credit to 1087 borrowers who are unlikely to default and give credit to 477 borrowers who are likely to default by using this model.

# 5 Final Remarks

Real-world models would be more complicated than these, and may take into account qualitative variables as well. However, this give a baseline to how well logistic regression models would work in binary classification problems. It is important to note that the models in this project do not consider a cost to misclassification, which is an important factor in real-world problems. There could also be a trade-off involved in the costs of false positives and false negatives and there would be a need to weigh these costs to select the best model to be implemented.

# References

<https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/>

<https://towardsdatascience.com/assumptions-of-logistic-regression-clearly-explained-44d85a22b290>

<https://www.statology.org/assumptions-of-logistic-regression/>

<https://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/>

<https://ademos.people.uic.edu/Chapter22.html>

<https://bookdown.org/tpinto_home/Regularisation/lasso-regression.html>
