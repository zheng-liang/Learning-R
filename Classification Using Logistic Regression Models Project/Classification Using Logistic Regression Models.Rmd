---
title: "Classification Using Logistic Regression Models"
output: 
  github_document:
    toc: true
---

# 1 Introduction

The purpose of this project was to apply what I had learnt about logistic regression and classification in machine learning (ML). It documents (1) the packages and functions that are useful for logistic classification and ML, and (2) the steps to building a logistic classification model in R.

The project has three main parts. In the first part, I introduced what logistic regression is and why it is used in classification in machine learning. In the second part, I provided examples of logistic regression in binary classification (i.e. there are only two outcomes) using an in-built dataset in R. In the third part, I attempted to use logistic regression in multinomial classification (i.e. there are more than two outcomes) using an in-built dataset as well.

(Intro to be further refined at the end)

## 1.1 What is logistic regression and why is it used in classification problems?

## 1.2 What are the assumptions needed for logistic regression?

This sub-section lists the common assumptions of logistic regressions and some of the ways to check them.

### 1.2(a) Type of logistic regression matches the response variable

The type of logit regression needs to match the type of outcome. In most cases, logit regression is used for binary outcomes but there are cases where it might be used for multinomial or ordinal outcomes and appropriate models need to be used for each type.

**To check assumption**: Check the number of unique outcomes/response variable. If there are only two outcomes, it is very likely to be a binary classification problem. If there are more than two outcomes, we are dealing with multinomial or ordinal classification problems.

### 1.2(b) Observations are independent

Observations need to be explanatory of one another, and not be affected by other observations

**To check assumption**: Plot the residuals against order of observations and check if there is a random pattern. If pattern is not random, it indicates possibility of correlation between observations. However, autocorrelation should be less of a concern with cross-sectional data as long as the design of the study ensures that data are collected from random samples and there are no repeated or paired observations.

### 1.2(c) No multicollinearity among explanatory variables

Independent variables should not be highly correlated with one another as it affects the variance of estimated coefficients and statistical significance of estimated coefficients becomes unreliable.

**To check assumption**: A simple method would be to use a correlation matrix to find highly correlated explanatory variables. Another method is to calculate the variance inflation factor (VIF) of each explanatory variable. A common rule of thumb is that VIF more than 10 indicates problem of multicollinearity.

### 1.2(d) Linear relationship between continuous explanatory variables and logit of response variable

Continuous explanatory variables and the logit of the response variable (or log-odds) have a linear relationship. Logit regression does not require linearity between continuous explanatory variables and response variable.

**To check assumption**: Plot log-odds against each continuous explanatory variable and check for linear relationship

### 1.2(e) No highly influential outliers

There should be no influential outliers that could affect the outcome of the model. Removing such observations are possible if they are determined to be incorrectly entered or measured. It is also possible to compare the regression results with and without the outliers, and note how the results differ.

**To check assumption**: Calculate Cook's Distance to find influential observations and standardized residuals to find outliers.

### 1.2(f) Large sample size

A large sample size is needed to produce conclusive results.

# 2 Packages Required

```{r load packages, message=FALSE}
library(car) 
library(caret) # For building ML models
library(corrplot) # For correlation plot
library(tidyverse) # For ggplot and dplyr packages
library(mlbench) # For data sets
library(lmtest) # For statistical tests
```

These packages provide the necessary functions and data sets that can be used for learning and building logistic classification models.

# 3 Basics of Logistic Regression

To start off with the basics of logistic regression in R, it is best to learn how it works with binary outcomes first. Such outcomes could be 0 or 1, Yes or No, Male or Female, etc. with only 2 possible outcomes. I selected the Pima Indians Diabetes Database in the **`mlbench`** package for this example. The purpose is to determine the probability of a person having diabetes based on several variables such as the person's body mass index, age, and blood pressure.

## 3.1 Exploratory Data Analysis

Let us first load the data and perform an Exploratory Data Analysis before the modelling task.

```{r load PimaIndiansDiabetes dataset}
data("PimaIndiansDiabetes")
```

```{r structure and summary}
str(PimaIndiansDiabetes)

summary(PimaIndiansDiabetes)
```

```{r head and tail of df}
head(PimaIndiansDiabetes, n = 4)

tail(PimaIndiansDiabetes, n = 4)
```

By entering `??PimaIndiansDiabetes` into the console, we can get a description of the variables in the data frame. I have listed the variables and its description in the table below.

| Variable | Description                                           |
|----------|-------------------------------------------------------|
| pregnant | Number of times pregnant                              |
| glucose  | Plasma glucose concentration (glucose tolerance test) |
| pressure | Diastolic blood pressure (mm Hg)                      |
| triceps  | Triceps skin fold thickness (mm)                      |
| insulin  | 2-Hour serum insulin (mu U/ml)                        |
| mass     | Body mass index (weight in kg/(height in m)^2)        |
| pedigree | Diabetes pedigree function                            |
| age      | Age (years)                                           |
| diabetes | Class variable (test for diabetes)                    |

The **`str`** function shows that there are 768 observations with 9 variables and only the variable *`diabetes`* is a factor while all other variables are numeric.

The **`summary`** function reveals that there are possibly some errors in the data, with value of zero in variables *`glucose`*, *`pressure`*, *`triceps`*, and *`mass`*. I would remove these observations to keep things simple.

```{r filter out obs}
data <- PimaIndiansDiabetes %>%
  dplyr::filter(glucose != 0 & pressure != 0 & triceps != 0 & mass != 0)

summary(data)
```

It seems that *`insulin`* has values of zero as well, which could be due to not receiving insulin despite having diabetes.

```{r split response and explvar}
response <- subset(data, select = c(diabetes))

explvar <- subset(data, select = -c(diabetes))

summary(response)
summary(explvar)
```

Let us take a look at the distribution of the response and explvar variables in a univariate plot. This gives us a visual understanding of how the variables are distributed individually.

```{r dist of diabetic test, fig.width=6, fig.height=4.5, fig.align='center'}
ggplot(data = response) + 
  geom_bar(aes(x = diabetes, fill = diabetes)) + 
  ggtitle("Distribution of Outcomes from Diabetes Test") +
  xlab("Diabetes") + ylab("Frequency") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r boxplot of dependent variables, fig.width=6, fig.height=4.5, fig.align='center'}
par(mfrow = c(1, 4), mar = c(1, 3, 2, 1))

for (i in 1:4) {
  boxplot(explvar[,i], main = names(explvar)[i])
}

for (i in 5:8) {
  boxplot(explvar[,i], main = names(explvar)[i])
}
```

Plotting the explanatory variables using histograms allow us to visually check how the data is distributed. The plots show that most variables do not have a normal distribution, which we can further check with the Shapiro-Wilk's test.

```{r histogram of dependent variables, fig.width=6, fig.height=4.5, fig.align='center'}
par(mfrow = c(1, 4), mar = c(2, 3, 2, 1))

for (i in 1:4) {
  hist(explvar[,i], main = names(explvar)[i])
}

for (i in 5:8) {
  hist(explvar[,i], main = names(explvar)[i])
}
```

```{r shapirotest}
store_test <- list()

for (i in 1:8) {
  store_test <- cbind(store_test, shapiro.test(x = explvar[,i]))
}

store_test

store_test["p.value",] > 0.05
```

We can use multivariate plots to visualize how variables interact with one another. Using a correlation matrix heatmap, we can visualize the correlation between variables and check for the presence of multicollinearity between explanatory variables. Since the Shapiro-Wilk's test rejected the hypothesis of normal distribution for the explanatory variables, I would use the Spearman correlation instead of Pearson correlation.

```{r correlation matrix heatmap}
corrX <- cor(x = explvar, method = "spearman")

corrplot(corr = corrX, 
         method = "color", 
         addCoef.col = "black",
         title = "Correlation Between Explanatory Variables",
         mar = c(0, 0, 1, 0))
```

From the correlation matrix, there does not seem to be any highly correlated explanatory variables, which may satisfy our assumption that there is no multicollinearity.

Now that we have a better understanding of the variables in the dataset, we can proceed with the modelling process.

## 3.2 Building the Binary Logistic Regression Model

To start off, we can create a logistic regression model that uses all the explanatory variables.

```{r create logit model1}
# Full model
model1 <- glm(diabetes ~ ., 
              data = data, 
              family = binomial(link = "logit"))

summary(model1)
```

Variables *`pressure`*, *`triceps`*, *`insulin`*, and *`age`* has p-values more than the significance level of 5%. We may consider removing them from the equation and see if we get better results. The Likelihood Ratio test will be needed to test if the difference is significant, which is something like an F-test for Ordinary Least Squares regression models. The null hypothesis is that the full model and the restricted model fits the data equally well and we should use the restricted model. The alternative is to use the full model.

```{r create logit model2}
# Restricted model
model2 <- glm(diabetes ~ pregnant + glucose + mass + pedigree,
              data = data,
              family = binomial(link = "logit"))

summary(model2)
```

```{r lrtest}
lmtest::lrtest(model2, model1)
```

Since the **`lrtest`** function returned a p-value more than 5%, we do not reject the null and hence, we will use the restricted model for our analysis.

### 3.2(a) Testing Multicollinearity Assumption

In Section 3.1 Exploratory Data Analysis, I used the correlation matrix to determine if there are any highly correlated explanatory variables. Another method is to use the Variance Inflation Factor.

```{r vif}
car::vif(model2)
```

None of the variables has a VIF of more than 10. Therefore, it is likely that there is no multicollinearity among the explanatory variables.

### 3.2(b) Testing Linearity Assumption

To check for if there is a linear relationship between the continuous explanatory variables and the logit of the response variable, we need to obtain the fitted probabilities from our model.

```{r fitted values from model2}
prob <- fitted.values(model2)
```

```{r calculate logit and bind into data}
pred <- names(coef(model2)[-1]) # Exclude intercept

plotdata <- subset(data, select = pred) # Only keep the variables used in the model

plotdata <- plotdata %>%
  mutate(logit = log(prob/(1-prob))) %>%
  gather(key = "pred", value = "pred.value", -logit) # Bind logit to plotdata
```

```{r plot variables against logit, message=FALSE, fig.align='center'}
ggplot(data = plotdata, aes(x = logit, y = pred.value)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "loess") +
  facet_wrap(. ~ pred, scales = "free")
```

We can see that variables *`glucose`*, *`mass`* and *`pregnant`* are have a rather linear relationship, but *`pedigree`* has a non-linear relationship with the log-odds. I have attempted to transform the variable using the natural log.

```{r create logit model3}
model3 <- glm(diabetes ~ pregnant + glucose + mass + log(pedigree),
              data = data,
              family = binomial(link = "logit"))

summary(model3)
```

```{r store fitted values from model3 and calculate logit}
prob2 <- fitted.values(model3)

logit = log(prob2/(1-prob2))
```

```{r transfrom pedigree with log}
plotdata2 <- subset(data, select = pred)

plotdata2 <- plotdata2 %>%
  mutate(l.pedigree = log(pedigree), pedigree = NULL)
```

```{r plot logped against logit, fig.width=6, fig.height=4.5, fig.align='center'}
ggplot(data = plotdata2, aes(x = logit, y = l.pedigree)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "loess")
```

After the log transformation, *`pedigree`* has a more linear relationship with the log-odds compared to before.

### 3.2(c) Checking for Highly Influential Outliers







# 4 Logistic Regression for Binary Classification






# 5 Logistic Regression for Multinomial Classification






# 6 Logistic Regression for Ordinal Classification















# References

<https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/>

<https://towardsdatascience.com/assumptions-of-logistic-regression-clearly-explained-44d85a22b290>

<https://www.statology.org/assumptions-of-logistic-regression/>

<https://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/>

https://ademos.people.uic.edu/Chapter22.html



