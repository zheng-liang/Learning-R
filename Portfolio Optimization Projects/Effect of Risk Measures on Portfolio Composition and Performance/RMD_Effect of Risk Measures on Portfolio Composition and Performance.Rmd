---
title: "Effect of Risk Measures on Portfolio Composition and Performance"
author: "Tan Zheng Liang"
date: "2022-07-09"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    code_folding: show
    highlight: tango
    theme: flatly
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center")
knitr::knit_hooks$set(purl = knitr::hook_purl)
```

Updated on: `r Sys.Date()`

## 1 Introduction

The purpose of this project is to determine the effect of different risk measures on portfolio performance. This project uses the Variance/Standard Deviation, Semi-Variance/Semi-Deviation, Downside Deviation (DD), Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) as the measures of risk for comparison.

The first part introduces the different risk measures using a single security. The second part shows how the different risk measures are used to optimize a portfolio with more than one security for a single-period. I first created a category of portfolios that minimizes the different risk measures, then created a category of optimal portfolios that minimizes the different risk measures for a given level of return. In each category of portfolios, I compared their performance and charted their cumulative returns.

The results in this project should not be taken as investment advice.

## 2 Packages Required

```{r load packages, message=FALSE, warning=FALSE}
library(dplyr)
library(doParallel) # For parallel computation in foreach loops
library(PerformanceAnalytics) # For portfolio performance and risk analysis
library(PortfolioAnalytics) # For portfolio optimization and analysis
library(quantmod) # For obtaining historical prices from Yahoo Finance
```

## 3 Understanding Risk Measures {.tabset .tabset-pills}

In this section, the different risk measures are introduced, starting with Variance/Standard Deviation as it is the fundamental risk measure of the Mean-Variance Framework developed by Harry Markowitz. Subsequently, I described the Semi-Variance/Semi-Deviation and Downside Deviation that measures only the downside risk or the negative fluctuations, followed by VaR and CVaR which measures the tail risk. The **`PerformanceAnalytics`** package provides the necessary functions for the different risk measures.

### 3.1 Retrieve MSFT Data

I used the stock returns of Microsoft Corporation (MSFT) to illustrate the different risk measures, which can be obtained using the **`getSymbols`** function in **`quantmod`** package.

```{r msft price}
startdate <- "2010-01-01"
enddate <- "2022-05-31"

MSFT <- quantmod::getSymbols(Symbols = "MSFT", # Indicate a symbol that corresponds to stock in Yahoo Finance
                             src = "yahoo", # Indicate to search stock prices from Yahoo Finance
                             from = startdate, # Indicate a start date
                             to = enddate, # Indicate an end date
                             periodicity = "monthly", # Indicate the periodicity of prices to retrieve, can also be "daily" or "weekly"
                             auto.assign = F) # Indicate FALSE so that results are assigned to variable we indicate

head(MSFT, n = 4); tail(MSFT, n = 4)

colSums(is.na(MSFT)) # To check for NA values
```

The adjusted price would be used to calculate the monthly return since it has been adjusted for dividend and splits.

```{r msft returns}
MSFT_returns <- PerformanceAnalytics::Return.calculate(prices = MSFT[,6], # Use the 6th column of MSFT (Adjusted Price)
                                                       method = "discrete" # Calculate the arithmetic/discrete returns, use "log" for log/continuous returns
                                                       )[-1,] # Remove first row since first price observation cannot be used to calculate return

head(MSFT_returns, 4)
```

### 3.2 Variance and Standard Deviation

The variance measures the sum of squared deviations from the expected return and this can be written as:

$$
\sigma^2 = \sum^N_{n=1} \frac{(R_n - E(R))^2}{N}
$$

The standard deviation is the square root of the variance or $\sqrt{\sigma^2_i}$. We can calculate the standard deviation of returns using the function **`StdDev`**.

```{r stddev msft}
MSFT_stddev <- PerformanceAnalytics::StdDev(R = MSFT_returns)

MSFT_stddev
```

The value of 0.062 means that 68% of the expected monthly return would range between the mean return $\pm$ 0.062 (1 sd).

However, the variance and standard deviation as a measure of risk are typically countered by two arguments. Firstly, it assumes that returns are normally distributed, which is usually not satisfied when dealing with financial returns. Secondly, it penalizes downside and upside risks equally, but investors are usually more concerned with downside risks than upside risks. In optimizing portfolio composition, the Mean-Variance Framework limits not just the losses, but also the gains. Therefore, different measures of risk have been developed to capture the left side of the distribution instead.

```{r distribution of msft returns, fig.align='center'}
PerformanceAnalytics::chart.Histogram(MSFT_returns, 
                                      main = "Distribution of MSFT Monthly Returns",
                                      methods = c("add.density", "add.normal"))

PerformanceAnalytics::skewness(MSFT_returns, method = "sample")

PerformanceAnalytics::kurtosis(MSFT_returns, method = "sample")
```

The daily return distribution of MSFT is slightly positively skewed, and has a higher kurtosis of 3.54 than the normal distribution of 3. Majority of the daily returns are concentrated around the mean as can be seen in the chart, but the tails are fatter, meaning that there can be extremely large monthly losses or gains. Hence, the monthly returns of MSFT may not normally distributed (can be confirmed with a formal test, such as Jarque-Bera or Shapiro-Wilks Test).

### 3.3 Semi-Variance and Semi-Deviation

The semi-variance and semi-deviation is similar to the variance and standard deviation, except that it only measures deviations below the expected return or the downside risk. The semi-deviation formula is written as:

$$
\sigma_{semi} = \sqrt{\sum_{n=1}^N \frac{\min(R_n - E(R), 0)^2}{N}} ~, \text{where N = total number of observations}
$$

We can calculate the semi-deviation of MSFT daily returns using the function **`SemiDeviation`**.

```{r semid msft}
MSFT_semidev <- PerformanceAnalytics::SemiDeviation(MSFT_returns)

MSFT_semidev
```

By using the semi-deviaion as a measure of risk, MSFT now has lower risk than if we had used the standard deviation (0.043 < 0.062). However, we are unable to use semi-deviation to determine the range of values that the daily returns would fall under a given probability, as we did using the standard deviation.

### 3.4 Downside Deviation

The downside deviation is similar to the semi-deviation, but it measures deviations below a target return (called the Minimum Acceptable Return or MAR). The formula is written as:

$$
DD = \sqrt{\sum_{n=1}^N \frac{\min(R_n - \text{MAR}, 0)^2}{N}} ~, \text{where N = total number of observations}
$$

Common values of MAR include the risk-free rate or zero, and if the mean of the returns is used, we would calculate the semi-deviation. Throughout this project, the MAR that I would use is 3%, which is approximately the yearly inflation rate.

```{r dd msft}
MSFT_downdev <- PerformanceAnalytics::DownsideDeviation(MSFT_returns, MAR = 0.03/12) # divide by 252 to change to daily periodicity

MSFT_downdev
```

Because the MAR is lesser than the expected MSFT daily return, we should obtain a smaller DD value than the semi-deviation. The DD might be a better measure of risk than the semi-deviation since the expected return may not be a good measure of the MAR.

### 3.5 Value-at-Risk

VaR measures the amount of loss that could occur over a specific time period, given a confidence level. For example, a 95% VaR of \$1 million with a time period of 1 month means that we are 95% confident that the loss would not exceed \$1 million. We can also rephrase it as we are 5% confident that the loss is greater than \$1 million over 1 month. The plot shows how VaR is indicated on a distribution of returns:

```{r plot VaR, fig.align='center'}
PerformanceAnalytics::chart.Histogram(MSFT_returns, 
                                      main = "Distribution of MSFT Monthly Returns with VaR",
                                      methods = c("add.risk")) # Indicate to show VaR and Modified VaR
```

There are generally three basic measures of VaR, namely the Historical VaR (HVaR), Parametric VaR (PVaR) and Modified VaR. 

For 95% HVaR, we determine the value of actual return distribution at the 5% probability. As it uses the historical returns to calculate the VaR, HVaR does not make any assumptions about the distribution of returns.

```{r HVaR msft}
MSFT_hvar <- PerformanceAnalytics::VaR(MSFT_returns,
                                       p = 0.95, # confidence level of 95%
                                       method = "historical") # use historical VaR

MSFT_hvar
```

Based on the HVaR, we are 95% confident that the monthly loss would not exceed 0.079 (or 7.9%). The statements "We are 95% confident that MSFT daily gain will be at least -0.079" and "We are 5% confident that MSFT monthly loss would exceed 0.079" are equivalent.

For 95% PVaR, we also determine the value of the return distribution at 5% probability, but we assume that the returns are normally distributed. PVaR is also known as the variance-covariance VaR.

```{r PVaR msft}
MSFT_pvar <- PerformanceAnalytics::VaR(MSFT_returns,
                                       p = 0.95, 
                                       method = "gaussian") # gaussian (normal) distribution

MSFT_pvar
```

Based on the PVaR, we are 95% confident that the monthly loss in MSFT would not exceed 0.082.

Modified VaR (also called Cornish-Fisher VaR) accounts for the skewness and kurtosis of the return distribution, which should be preferred when returns are not normally distributed.

```{r modVaR msft}
MSFT_modvar <- PerformanceAnalytics::VaR(MSFT_returns, 
                                         p = 0.95, 
                                         method = "modified") # use modified VaR

MSFT_modvar
```

Based on the Modified VaR, we are 95% confident that the monthly loss in MSFT would not exceed 0.080.

While VaR allows us to determine the amount of loss that could occur within a time period at a given confidence interval, it does not actually quantify the loss that could occur beyond the VaR.

### 3.6 Conditional Value-at-Risk

CVaR, also known as the expected shortfall (ES) or expected tail loss (ETL) measures the loss occurring beyond the VaR. Similar to VaR, the calculation of CVaR can be based on the historical, parametric, or modified methods. In the **`PerformanceAnalytics`** package, the functions **`CVaR`**, **`ES`** and **`ETL`** are equivalent. For a 95% CVaR, we calculate the average of the 5% tail loss. 

```{r historical CVaR msft}
MSFT_histcvar <- PerformanceAnalytics::CVaR(MSFT_returns, 
                                            p = 0.95, 
                                            method = "historical")

MSFT_histcvar
```

Based on the historical method of CVaR, the average loss in the 5% tail of the return distribution is 0.106 or 10.6%.

```{r parametric CVaR msft}
MSFT_parcvar <- PerformanceAnalytics::CVaR(MSFT_returns,
                                           p = 0.95, 
                                           method = "gaussian")

MSFT_parcvar
```

Based on the parametric CVaR, the average loss in the 5% tail of the return distribution is 0.108.

```{r modified CVaR msft}
MSFT_modcvar <- PerformanceAnalytics::CVaR(MSFT_returns, 
                                           p = 0.95, 
                                           method = "modified")

MSFT_modcvar
```

Based on the modified CVaR, the average loss in the 5% tail of the return distribution is 0.108.

### 3.7 Summary of Risk Measures

In this section, we have seen the different variations of risk measures, with the standard deviation, semi-deviation and downside deviation measuring fluctuations of returns around or below a specified value and VaR and CVaR measuring the tail losses.

Here are the values of the risk measures calculated in this section:

```{r summary of risk measures}
c(Std_Dev = MSFT_stddev, Semi_Dev = MSFT_semidev, Downside_Dev = MSFT_downdev)

c(H_VaR = MSFT_hvar, P_VaR = MSFT_pvar, Mod_VaR = MSFT_modvar,
  H_CVaR = MSFT_histcvar, P_CVaR = MSFT_parcvar, Mod_CVaR = MSFT_modcvar)
```

For the VaR and CVaR calculations, it is assumed that historical values can determine future patterns of returns since we are using the actual historical data. Hence, VaR and CVaR have been countered by arguments of its viability on future data and events.

## 4 Retrieving Data For Portfolio

In this section, we start applying the different risk measures into the portfolio optimization problem. I have randomly chosen a few large-cap stocks listed in the U.S. for the portfolio optimization:

* Apple Inc (AAPL)
* Microsoft Corp. (MSFT)
* Johnson & Johnson (JNJ)
* Procter & Gamble (PG)
* Mastercard Incorporated (MA)
* Home Depot, Inc. (HD)
* Pfizer, Inc. (PFE)
* Merck & Company, Inc. (MRK)
* Costco Wholesale Corporation (COST)
* NVIDIA Corporation (NVDA)

I have retrieved the historical price data of each ETF, keeping only the adjusted price column. I have used the same start and end date for data retrieval as Section 3.

```{r portfolio price data}
tickers <- c("AAPL", "MSFT", "JNJ", "PG", "MA", "HD", "PFE", "MRK", "COST", "NVDA")

price_data <- NULL

for (ticker in tickers) {
  price_data <- cbind(price_data,
                      quantmod::getSymbols(ticker, 
                                           src = "yahoo", 
                                           from = startdate, to = enddate, 
                                           periodicity = "monthly", 
                                           auto.assign = F)[,6])
}

colnames(price_data) <- tickers
```

Let us take a look at the `price_data` object we have created to make sure that the data is correctly retrieved.

```{r head tail of price data}
head(price_data, n = 4); tail(price_data, n = 4)

nrow(price_data); colSums(is.na(price_data))
```

We have a total of 149 monthly observations per security and there are no NA values in the columns. Now that we have ensured the price data is in time order and there are no missing values, we can calculate the monthly returns of each security.

```{r individual security return}
returns <- na.omit(PerformanceAnalytics::Return.calculate(price_data, method = "discrete"))

head(returns); nrow(returns)
```

With the `returns` data, we can proceed with optimizing the portfolios based on different risk measures.

## 5 Portfolio Specification and Random Portfolios {.tabset .tabset-pills}

### 5.1 Create Portfolio Specification

Before optimizing portfolios, I have created an initial portfolio specification with common constraints. It includes a sum of weight and individual asset weight constraints, which can be used throughout the different portfolio optimization problems.

```{r initial portfolio spec}
init_spec <- PortfolioAnalytics::portfolio.spec(assets = colnames(returns))

# Sum of weights constrained to 1 but require some "wiggle" room for random portfolio generation
init_spec <- add.constraint(portfolio = init_spec, 
                            type = "weight_sum", 
                            min_sum = .99, max_sum = 1.01)

# Weight of each asset is constrained to min of 0 and max of 20% of portfolio
init_spec <- add.constraint(portfolio = init_spec, 
                            type = "box", 
                            min = 0, max = 0.20)
```

While the `ROI` solver can be used with var/StdDev and ETL/ES/CVaR risk objectives, it cannot be used with VaR and other downside risk measures. As such, I have opted to use the random portfolios method for optimization. Generating random portfolios that satisfy the portfolio constraints and objectives may be a problem if a small number of permutation was chosen, and may not allow for satisfactory comparison of performance. To run a very large number of permutations would require time and computing power.

### 5.2 Create Random Portfolios

Create random portfolios satisfying the above constraints:

```{r random portfolios}
set.seed(1)

randport <- random_portfolios(portfolio = init_spec, 
                              permutations = 25000, 
                              rp_method = "sample",
                              eliminate = T) # Eliminate portfolios that do not meet requirements

dim(randport) # 24999 random portfolios found
```

With the random portfolios, I created a foreach loop to find the monthly returns of each random portfolio in `randport`. To speed things up, I ran the **`foreach`** loop in parallel using **`dopar`**.

Calculate portfolio returns of every weight composition in `randport`:

```{r random portfolios returns, warning=FALSE, message=FALSE}
# Important to make sure that we do not use all cores for computation. Check with detectCores(logical = F)
mycluster <- makeCluster(6) # Number of cores to use. I have 8, but may not be suitable for everyone.

registerDoParallel(mycluster) # Register parallel computing

total_time <- system.time(
output1 <- foreach(i = 1:nrow(randport), .combine = "cbind") %dopar% {
  
  rp_return <- PerformanceAnalytics::Return.portfolio(R = returns, 
                                                      weights = randport[i,], 
                                                      geometric = T,
                                                      rebalance_on = "quarters")
  
  }
) # system.time to measure time take to run the loop

stopCluster(mycluster) # End parallel computing

total_time

dim(output1)
```

We can see that there are 148 rows and 24999 columns, corresponding to number of observations in `returns` and number of random portfolios in `randport` respectively. The `total_time` shows the time taken to calculate the 24,999 random portfolio returns. The time elapsed (in seconds) gives us an idea of the amount of time needed to calculate returns of all random portfolios.

## 6 Minimum Risk Portfolios {.tabset .tabset-pills}

In this section, I would start by creating the minimum variance portfolio, followed by portfolios with minimum semi-variance, downside deviation, VaR and CVaR. The creation of these portfolios require the package **`PortfolioAnalytics`**. 

### 6.1 Minimum Variance Portfolio

With the monthly returns of every random portfolio in `output1`, we can calculate the standard deviation and find the minimum to obtain the minimum variance portfolio.

Optimize portfolio based on minimum variance:

```{r minimize standard deviation}
stddev_search <- PerformanceAnalytics::StdDev(R = output1, portfolio_method = "single")

min(stddev_search)

weight_MinVar <- randport[which.min(stddev_search),]

weight_MinVar
```

### 6.2 Minimum Semi-Deviation Portfolio

Optimize portfolio based on minimum semi-deviation:

```{r minimize semi-deviation}
semidev_search <- PerformanceAnalytics::SemiDeviation(R = output1)

min(semidev_search)

weight_MinSemiDev <- randport[which.min(semidev_search),]

weight_MinSemiDev
```

### 6.3 Minimum Downside Deviation Portfolio

Optimize portfolio based on minimum downside deviation:

```{r minimize downside deviation}
downdev_search <- PerformanceAnalytics::DownsideDeviation(R = output1, MAR = 0.03/12)

min(downdev_search)

weight_MinDD <- randport[which.min(downdev_search),]

weight_MinDD
```

### 6.4 Minimum Value-at-Risk Portfolio

For VaR, I would use the historical VaR, although one could experiment with other VaR methods. Since the VaR measure is negative, we want to find the highest negative measure (i.e. closer to or more than 0). If VaR was framed as a positive measure, we would simply minimize the VaR.

Optimize portfolio based on minimum historical VaR:

```{r minimize historical VaR}
hvar_search <- PerformanceAnalytics::VaR(R = output1, p = 0.95, method = "historical")

max(hvar_search)

weight_MinHVaR <- randport[which.max(hvar_search),]

weight_MinHVaR
```

### 6.5 Minimum Conditional Value-at-Risk Portfolio

Similar to the VaR portfolio optimization, I would use the historical CVaR.

Optimize portfolio based on minimum historical CVaR:

```{r minimize historical CVaR}
hcvar_search <- PerformanceAnalytics::CVaR(R = output1, p = 0.95, method = "historical")

max(hcvar_search)

weight_MinHCVaR <- randport[which.max(hcvar_search),]

weight_MinHCVaR
``` 

### 6.6 Summary {.tabset}

#### 6.6.1 Merge Portfolios

Now that we have created the different portfolios by minimizing different risk measures, we can compare the weights of their components, their performance and chart their cumulative returns.

```{r weights of minimum risk portfolios}
port_weights <- rbind(Min_Var = weight_MinVar, 
                      Min_SemiDev = weight_MinSemiDev, 
                      Min_DownDev = weight_MinDD, 
                      Min_HVaR = weight_MinHVaR, 
                      Min_HCVaR = weight_MinHCVaR)

port_weights
```

Since we have calculated the monthly returns of each portfolio in `output1`, we can extract it and save it into a suitable data frame.

```{r extract monthly returns of min risk portfolios}
return_comp <- cbind(output1[, which.min(stddev_search)],
                     output1[, which.min(semidev_search)],
                     output1[, which.min(downdev_search)],
                     output1[, which.max(hvar_search)],
                     output1[, which.max(hcvar_search)]) %>%
  `colnames<-`(c("Min_Var", "Min_SemiDev", "Min_DownDev", "Min_HVaR", "Min_HCVaR"))

head(return_comp)
```

#### 6.6.2 Benchmark

An appropriate benchmark would be needed if we wish to properly compare the performance of our portfolios. I am using the SPDR S&P 500 ETF, which tracks the S&P 500 Large-Cap Index.

```{r benchmark return}
benchmark <- quantmod::getSymbols(Symbols = "SPY",
                                  src = "yahoo", 
                                  from = startdate, to = enddate, 
                                  periodicity = "monthly", 
                                  auto.assign = F)[,6] %>%
  `colnames<-`("SPY")

benchmark_return <- na.omit(PerformanceAnalytics::Return.calculate(prices = benchmark, method = "discrete"))

head(benchmark_return); dim(benchmark_return)
```

#### 6.6.3 Plot Cumulative Return

After compiling the returns, we can plot their cumulative returns using **`chart.CumReturns`** and compare their drawdowns by using 

```{r plot performance chart of minimum risk portfolios, message=FALSE, warning=FALSE}
PerformanceAnalytics::chart.CumReturns(R = cbind(return_comp, benchmark_return), 
                                       geometric = T, # Use geometric chaining for portfolio cumulative returns over time
                                       main = "Cumulative Returns of Minimum Risk Portfolios", 
                                       plot.engine = "plotly")

PerformanceAnalytics::chart.Drawdown(R = cbind(return_comp, benchmark_return), 
                                     geometric = T,
                                     main = "Drawdown of Minimum Risk Portfolios", 
                                     plot.engine = "plotly")
```

The minimum risk portfolios all had better cumulative returns than the SPY benchmark. In descending order of cumulative returns based on the chart, `Min_HCVaR` > `Min_DownDev` > `Min_HVaR` > `Min_SemiDev` > `Min_Var` portfolios. In general, the SPY benchmark has larger drawdowns than the minimum risk portfolios.

#### 6.6.4 Performance Metrics and Statistics

```{r stats and annualized metrics of minimum risk portfolios}
table.AnnualizedReturns(R = cbind(return_comp, benchmark_return), scale = 12, Rf = 0.02/12, geometric = T, digits = 4)

table.Stats(R = cbind(return_comp, benchmark_return), digits = 4)

table.CAPM(Ra = return_comp, Rb = benchmark_return, scale = 12, Rf = 0.02/12, digits = 4)

table.DownsideRisk(R = cbind(return_comp, benchmark_return), scale = 12, Rf = 0.02/12, MAR = 0.03/12, digits = 4)
```

## 7 Optimal Portfolio Selection With Target Return {.tabset .tabset-pills}

### 7.1 Mean Return Target

In this section, I optimized portfolios based on a target return of 20% annually. Since we have generated random portfolios in Section 5, I used that to determine the subset of portfolios that meets the target return requirement. The reason for doing this is because random portfolios generated using **`random_portfolios`** function does not account for the return constraint.

From the `output1` object in Section 5 which contains the monthly return of all the random portfolios in `randport`, I would subset portfolios that have mean return of 20% (add some wiggle room instead of a hard target, 19.9% to 20.1%) and find portfolios that have the minimum of different risk measures.

```{r subset of randport with target return}
monthly_meanret <- PerformanceAnalytics::Mean.arithmetic(x = output1)

target_return <- monthly_meanret > 0.199/12 & monthly_meanret < 0.201/12

subrp_return <- output1[, target_return == "TRUE"]

dim(subrp_return)
```

Out of 24,999 random portfolios generated in Section 5, only 796 portfolios meet the target return constraint.

```{r subset randport weights}
sub_randport <- randport[target_return == "TRUE",]
```

### 7.2 Mean-Variance Portfolio

Optimize portfolio based on mean-variance:

```{r mean variance portfolio}
meanvar_search <- PerformanceAnalytics::StdDev(R = subrp_return, portfolio_method = "single")

min(meanvar_search)

weight_MeanVar <- sub_randport[which.min(meanvar_search),]

weight_MeanVar
```

### 7.3 Mean-Semi Deviation Portfolio

Optimize portfolio based on mean-semi deviation:

```{r mean semidev portfolio}
meansemidev_search <- PerformanceAnalytics::SemiDeviation(R = subrp_return)

min(meansemidev_search)

weight_MeanSemiDev <- sub_randport[which.min(meansemidev_search),]

weight_MeanSemiDev
```

### 7.4 Mean-Downside Deviation Portfolio

Optimize portfolio based on mean-downside deviation:

```{r mean downdev portfolio}
meandowndev_search <- PerformanceAnalytics::DownsideDeviation(R = subrp_return, MAR = 0.03/12)

min(meandowndev_search)

weight_MeanDownDev <- sub_randport[which.min(meandowndev_search),]

weight_MeanDownDev
```

### 7.5 Mean-VaR Portfolio

Optimize portfolio based on mean-VaR:

```{r mean VaR portfolio}
meanVaR_search <- PerformanceAnalytics::VaR(R = subrp_return, p = 0.95, method = "historical")

max(meanVaR_search)

weight_MeanVaR <- sub_randport[which.max(meanVaR_search),]

weight_MeanVaR
```

### 7.6 Mean-CVaR Portfolio

Optimize portfolio based on mean-CVaR:

```{r mean CVaR portfolio}
meanCVaR_search <- PerformanceAnalytics::CVaR(R = subrp_return, p = 0.95, method = "historical")

max(meanCVaR_search)

weight_MeanCVaR <- sub_randport[which.max(meanCVaR_search),]

weight_MeanCVaR
```

### 7.7 Summary {.tabset}

#### 7.7.1 Merge Portfolios

With the different optimal portfolios created, we can now compare their weights, performance and chart their cumulative returns.

```{r weights of mean risk portfolios}
weight_comp <- rbind(Mean_Var = weight_MeanVar,
                     Mean_SemiDev = weight_MeanSemiDev,
                     Mean_DownDev = weight_MeanDownDev,
                     Mean_VaR = weight_MeanVaR,
                     Mean_CVaR = weight_MeanCVaR)

weight_comp
```

Again, based on the set of random portfolios generated, the `Mean_Var` and `Mean_SemiDev` portfolios have the same asset weights.

We can extract the monthly returns of the selected portfolios from the `subrp_return` object.

```{r extract monthly returns of optimal portfolios}
return_comp <- cbind(subrp_return[, which.min(meanvar_search)],
                     subrp_return[, which.min(meansemidev_search)],
                     subrp_return[, which.min(meandowndev_search)],
                     subrp_return[, which.max(meanVaR_search)],
                     subrp_return[, which.max(meanCVaR_search)]) %>%
  `colnames<-`(c("Mean_Var", "Mean_SemiDev", "Mean_DownDev", "Mean_VaR", "Mean_CVaR"))

head(return_comp)
```

#### 7.7.2 Plot Cumulative Return

```{r plot performance chart of optimal portfolios, message=FALSE, warning=FALSE}
PerformanceAnalytics::chart.CumReturns(R = cbind(return_comp, benchmark_return), 
                                       geometric = T, # Use geometric chaining for portfolio cumulative returns over time
                                       main = "Cumulative Returns of Optimal Portfolios", 
                                       plot.engine = "plotly")

PerformanceAnalytics::chart.Drawdown(R = cbind(return_comp, benchmark_return), 
                                     geometric = T,
                                     main = "Drawdown of Optimal Portfolios",
                                     plot.engine = "plotly")
```

With an additional constraint of having 20% target return, the optimal portfolios have similar cumulative returns (since we added a target return). The SPY benchmark still has larger drawdowns compared to the optimal portfolios in general.

#### 7.7.3 Performance Metrics and Statistics

Below are the common statistics and performance metrics that can be compared to the SPY benchmark.

```{r stats and annualized metrics of optimal portfolios}
table.AnnualizedReturns(R = cbind(return_comp, benchmark_return), scale = 12, Rf = 0.02/12, geometric = T, digits = 4)

table.Stats(R = cbind(return_comp, benchmark_return), digits = 4)

table.CAPM(Ra = return_comp, Rb = benchmark_return, scale = 12, Rf = 0.02/12, digits = 4)

table.DownsideRisk(R = cbind(return_comp, benchmark_return), scale = 12, Rf = 0.02/12, MAR = 0.03/12, digits = 4)
```

## 8 Final Remarks

While variance and standard deviation have been commonly used as a measure of risk, other downside risk measures may be better suited to quantify the risks that investors face. In Section 5, we see that the other minimum risk portfolios have higher cumulative returns than the minimum variance portfolio, but had drawdowns that were generally between that of the minimum variance portfolio and the SPY benchmark. In Section 6, the mean-variance portfolio had similar cumulative returns and drawdowns compared to the other optimal portfolios of different risk measures. This might be due to the limited permutations of random portfolios generated, which stems from the limited computational power and the expected time I was willing to wait for the random portfolios to be created.

It is important to note that the results are only applicable to the random portfolios I had generated, which can be replicated via the `set.seed` when I created the random portfolios. By increasing the number of permutations, one would likely find another set of portfolios that minimizes the different risk measures.

## References

<https://analystprep.com/study-notes/frm/part-1/valuation-and-risk-management/measures-of-financial-risk/>

<https://corporatefinanceinstitute.com/resources/knowledge/trading-investing/value-at-risk-var/>

<https://www.financialplanningassociation.org/article/journal/JUN13-intuitive-examination-downside-risk>

<https://www.investopedia.com/terms/s/semideviation.asp>

<https://pages.stern.nyu.edu/~adamodar/pdfiles/valrisk/ch4.pdf>
