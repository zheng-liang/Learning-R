---
title: "Non-Stationary Processes in Time Series Analysis"
author: "Tan Zheng Liang"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: yes
    toc_float: yes
    code_folding: show
    highlight: tango
    theme: flatly
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center", message = FALSE, warning = FALSE)
knitr::knit_hooks$set(purl = knitr::hook_purl)
```

## 1 Introduction

The purpose of this project was to understand non-stationary processes in time series analysis and the difference between deterministic and stochastic trend in time series variables. It also sought to find out if the treatment for achieving stationarity (differencing, de-trending, including trend in regression) affects the accuracy of out-of-sample forecast for time series with a deterministic trend.

The project used a simulation of random walk and ARIMA models and added a drift and/or trend term to simulate non-stationary time series data.

## 2 Packages Required

```{r load packages}
library(forecast) # For ARIMA modelling, forecasting and evaluation
library(urca) # For unit root and stationarity tests
```

## 3 Non-Stationary Processes {.tabset .tabset-pills}

In this section, the different non-stationary processes are introduced, along with the simulation of the processes.

### 3.1 Random Walk

I began the analysis of non-stationary processes with the random walk (RW) model as this model will lay the foundation for the models discussed later.

The RW model has the form $y_t = y_{t-1} + \varepsilon_t$, where $\varepsilon_t \sim N(0, \sigma^2)$. This means that the current value is based on its previous value and a stochastic white noise component.

The model can be easily rewritten with the form $y_t = y_0 + \sum_{s=1}^t \varepsilon_s$ with $E(y_t) = y_0$ and $V(y_t) = t \sigma^2$. The mean is dependent on $y_0$ and without loss of generality, $y_0 = 0$ means that $y_t$ is random and cannot be accurately predicted. The variance is dependent on and increases with time.

Using **`stats::arima.sim()`**, we can simulate the RW model as such:

```{r simulate RW}
# Number of observations
N = 500

# Model to simulate
# For RW model, only need d = 1 in (p,d,q) to include unit root
M = list(order = c(0, 1, 0))

set.seed(123)

RW <- stats::arima.sim(model = M, n = N)
                       
# Plot RW
plot(x = RW, main = "Random Walk", ylab = "y", lwd = 2)
```

Using **`urca::ur.df()`** and **`urca::ur.kpss`**, the Augmented Dickey Fuller test for unit root and KPSS test for stationarity can be performed:

```{r ADF and KPSS tests for RW}
RW %>% urca::ur.df(type = "none", selectlags = "AIC") %>% summary()

# KPSS requires a deterministic component, either drift (mu) or trend (tau)
RW %>% urca::ur.kpss(type = "mu", lags = "short") %>% summary()
```

The null hypothesis of the ADF test cannot be rejected (there is unit root) and the null hypothesis of the KPSS test was rejected (not stationary).

### 3.2 Random Walk with Drift

The second non-stationary process is a random walk model with drift (or constant). When plotted, the RW model with drift would look like a trending series. A positive/negative constant would produce a upward/downward trending RW series.

This model can be represented by $y_t = \alpha + y_{t-1} + \varepsilon_t$, where $\varepsilon_t \sim N(0, \sigma^2)$. This is similar to the RW model, with the current value also affected by a drift (or constant) term.

It is easily shown that the model can be rewritten as $y_t = t \alpha + y_0 + \sum_{s = 1}^t \varepsilon_s$, with $E(y_t) = t \alpha + y_0$ and $V(y_t) = t \sigma^2$. WLOG, $y_0 = 0$ shows that the mean is increasing with time. Same as the RW model, the variance of the RW model with drift is time-dependent and increases with time.

Using **`stats::arima.sim()`**, we can simulate the RW model with drift as such:

```{r simulate RW with drift}
# Use same number of observations (N) and model (M) as RW model

# Include a non-zero mean value and optionally the standard deviation
# RW with positive constant
set.seed(234)
RW_posDrift <- stats::arima.sim(model = M, n = N, mean = 0.5, sd = 10)

# RW with negative constant
set.seed(234)
RW_negDrift <- stats::arima.sim(model = M, n = N, mean = -0.5, sd = 10)

# Plot RW with drift
plot(x = cbind(RW_posDrift, RW_negDrift), 
     main = "Random Walk with Drift", ylab = "y", 
     plot.type = "single", 
     col = c("black", "red"), lwd = 2)

legend(x = "topleft", legend = c("RW with positive drift", "RW with negative drift"), 
       col = c("black", "red"), lwd = 2)
```

Perform ADF and KPSS tests for the RW model with positive constant:

```{r ADF and KPSS tests for RW with drift}
RW_posDrift %>% urca::ur.df(type = "drift", selectlags = "AIC") %>% summary()

RW_posDrift %>% urca::ur.kpss(type = "mu", lags = "short") %>% summary()
```

The null hypothesis of the ADF test cannot be rejected (there is unit root) and the null hypothesis of the KPSS test was rejected (not stationary).

### 3.3 Deterministic Trend

A simple model with a deterministic trend represented by $y_t = \delta t + \varepsilon_t$, where $\varepsilon_t \sim N(0, \sigma^2)$, is different from stochastic trend processes such as the RW model with and without drift. This model is non-stationary because a time component is involved, with $E(y_t) = \delta t$ such that the mean is time-dependent. By removing the time trend (de-trending), the series would be stationary since $\varepsilon_t$ is a white noise process.

When plotted, values still vary up and down but returns to the trend, without ever drifting off from the trend permanently.

To simulate a model with deterministic trend, a time trend needs to be created:

```{r simulate simple model with deterministic trend}
# Use same number of observations as RW model, observation begans from time 0
tt <- 0:N

set.seed(345)

# Set sd = 20 so that the ups and downs can be seen clearly
DT <- as.ts(tt + rnorm(n = N, mean = 0, sd = 10))

plot(x = DT, main = "Deterministic Trend", ylab = "y", lwd = 2)
```

Perform ADF and KPSS tests:

```{r ADF and KPSS tests for model with DT}
DT %>% urca::ur.df(type = "trend", selectlags = "AIC") %>% summary()

DT %>% urca::ur.kpss(type = "tau", lags = "short") %>% summary()
```

The null hypothesis of the ADF test was rejected (there is no unit root) and the null hypothesis of the KPSS test cannot be rejected (stationary with deterministic trend).

### 3.4 Random Walk with Drift and Deterministic Trend

Lastly, we can have a non-stationary process combining both deterministic and stochastic trend terms. A simple example of this process is a random walk with drift and deterministic trend with the form $y_t = \alpha_0 + \delta t + y_{t-1} + \varepsilon_t$, where $\varepsilon_t \sim N(0, \sigma^2)$. Recursively expanding the lags of $y_t$ in the equation yields $y_t = t \alpha_0 + \delta (t + (t-1) + (t-2) + \dotsc + 3 + 2 + 1) + y_0 + \sum_{s=1}^t \varepsilon_s$. The expected mean is then $E(y_t) = t \alpha_0 + \delta \bigg( \frac{t(t+1)}{2} \bigg) + y_0$ and the variance is $V(y_t) = t \sigma^2$. Both the mean and variance depends on time.

Simulating a random walk with drift and deterministic trend requires a time trend and a RW model with drift:

```{r simulate RW with drift and DT}
# Create simulated model by adding time trend (tt) to RW_posDrift
RW_posDrift_DT <- tt + RW_posDrift

plot(x = RW_posDrift_DT, main = "Random Walk with Drift and Deterministic Trend", ylab = "y", lwd = 2)
```

Perform ADF and KPSS tests:

```{r ADF and KPSS tests for RW with drift and DT}
RW_posDrift_DT %>% urca::ur.df(type = "trend", selectlags = "AIC") %>% summary()

RW_posDrift_DT %>% urca::ur.kpss(type = "tau", lags = "short") %>% summary()
```

The null hypothesis of the ADF test cannot be rejected (there is unit root) and the null hypothesis of the KPSS test was rejected (not stationary).

### 3.5 Summary

Plot the RW, RW with positive drift, deterministic trend and RW with positive drift and deterministic trend:

```{r plot models in same chart}
plot(cbind(RW, RW_posDrift, DT, RW_posDrift_DT),
     main = "Non-Stationary Processes", ylab = "y", 
     plot.type = "single", 
     col = c("black", "red", "blue", "green"), lwd = 2)

legend(x = "topleft",
       legend = c("RW", "RW with positive drift", "Deterministic trend", "RW with positive drift and deterministic trend"), 
       col = c("black", "red", "blue", "green"), lwd = 2)
```

The ADF and KPSS tests were conducted for each model and only the model with deterministic trend was stationary after accounting for the time trend in the tests (so it is a trend stationary process). In the next section, I discussed how non-stationary processes can be made stationary by differencing, de-trending or both.

## 4 Transform Non-Stationary to Stationary Processes {.tabset .tabset-pills}

In this section, I discussed how each of the non-stationary processes can be made stationary, in order of their presentation in Section 3.

### 4.1 Differencing

Time series with a stochastic trend, such as the RW with and without drift, can be transformed into a stationary process by taking differences. Based on the RW equation in Section 3.1, subtracting $y_{t-1}$ from both sides of the equation would yield:

$$y_t - y_{t-1} = y_{t-1} - y_{t-1} + \varepsilon_t \\ \Delta y_t = \varepsilon_t$$

Since $\varepsilon_t$ is a white noise process, the RW model is stationary after taking the first difference. Such variables are integrated of order one or $I(1)$. If a variable is not stationary after taking the first difference, but stationary after the second difference, it is an $I(2)$ variable and so on.

Taking the first difference of RW and RW with drift:

```{r differencing RW and RW with drift}
RW_diff <- diff(x = RW, lag = 1, differences = 1)

RWdrift_diff <- diff(x = RW_posDrift, lag = 1, differences = 1)

plot(x = cbind(RW_diff, RWdrift_diff), 
     main = "First-Differenced Random Walk with and without Drift", 
     ylab = "Differenced y", plot.type = "single",
     col = c("black", "red"), lwd = 2)

legend(x = "topleft", 
       legend = c("Differenced RW", "Differenced RW with drift"), 
       col = c("black", "red"), lwd = 2)
```

Perform ADF and KPSS tests on differenced RW:

```{r ADF and KPSS test of differenced RW}
RW_diff %>% urca::ur.df(type = "drift", selectlags = "AIC") %>% summary()

RW_diff %>% urca::ur.kpss(type = "mu", lags = "short") %>% summary()
```

The null hypothesis of the ADF test was rejected (no unit root) and the null hypothesis of the KPSS test cannot be rejected (stationary). The test results would be the same for the differenced RW with drift.

### 4.2 De-trending

For time series variables with a deterministic trend, assumptions are needed for the time trend on whether it might be linear, exponential, or quadratic. Remove the time trend and the variable would be stationary. Based on the model with deterministic trend in Section 3.3, suppose we know the true data generating process, we can simply subtract a linear time trend from the equation, therefore leaving us with a stochastic white noise process:

$$y_t - \delta t = \varepsilon_t$$

Another way is to regress $y_t$ on a time trend and obtain the residuals (or subtract fitted $y_t$ from actual $y_t$), which should be a white noise process.

Plot time trend with the model before and after removing the deterministic trend:

```{r removing time trend from model}
DT_detrend <- DT - tt

plot(x = cbind(DT, tt, DT_detrend), 
     main = "Before and After Detrending", ylab = "y",
     plot.type = "single",
     col = c("black", "red", "blue"), lwd = 2)

legend(x = "topleft", legend = c("Deterministic trend", "Time Trend", "Detrend"),
       col = c("black", "red", "blue"), lwd = 2)
```

Perform ADF and KPSS tests on model after de-trending:

```{r ADF and KPSS tests on model after de-trending}
DT_detrend %>% urca::ur.df(type = "drift", selectlags = "AIC") %>% summary()

DT_detrend %>% urca::ur.kpss(type = "mu", lags = "short") %>% summary()
```

The null hypothesis of the ADF test was rejected (no unit root) and the null hypothesis of the KPSS test cannot be rejected (stationary).

### 4.3 Differencing a Trend Stationary Variable

Suppose that we subtracted the previous value of $y$ from its current value, where $y_{t-1} = \delta (t-1) + \varepsilon_{t-1}$. This would transform the trend stationary equation to $\Delta y_t = \delta + \Delta \varepsilon_{t-1}$. Although $\varepsilon_t$ is a stochastic white noise process, the transformed variable $\Delta \varepsilon_t = \varepsilon_t - \varepsilon_{t-1}$ introduces a problem. The shock at time $t$ does not only affect the current $y$ value but also the $y$ value next period and might not disappear as time passes.

Take first difference on model with deterministic trend:

```{r difference model with DT once}
DT_diff <- diff(x = DT, lag = 1, differences = 1)

# Remove first row of DT_detrend as differencing causes us to lose one observation
plot(x = cbind(DT_detrend[-1], DT_diff), 
     main = "De-trending vs Differencing of Deterministic Trend Process", ylab = "",
     plot.type = "single",
     col = c("black", "red"), lwd = c(2, 1), lty = c(1, 2))

legend(x = "topleft",
       legend = c("De-trended", "Differenced"),
       col = c("black", "red"), lwd = c(2, 1), lty = c(1, 2))
```

Perform ADF and KPSS tests for differenced variable with deterministic trend:

```{r ADF and KPSS tests for DT_diff}
DT_diff %>% urca::ur.df(type = "drift", selectlags = "AIC") %>% summary()

DT_diff %>% urca::ur.kpss(type = "mu", lags = "short") %>% summary()
```

The ADF and KPSS tests show that the differenced model with a deterministic trend is still stationary and has no unit root, but the standard deviation of the differenced variable is greater than the de-trended variable.

### 4.4 Combining De-trending and Differencing

For variables with a stochastic and deterministic trend, differencing is required to remove the stochastic trend and regressing the differenced variable on a time trend to obtain the residuals, which removes the deterministic trend.

With the RW with drift and deterministic trend in Section 3.4, we can first obtain the residual from regressing $\Delta y_t$ on $t$ and a constant, which removes the deterministic trend, then subtract $y_{t-1}$ to remove the stochastic trend, leaving behind the white noise process.

Transform RW with drift and deterministic trend into a stationary variable:

```{r stationarity of RW with drift and DT}
reg_y <- lm(formula = RW_posDrift_DT ~ tt)

delta_y <- diff(x = residuals(reg_y), lag = 1, differences = 1)

# Remove first observation from residuals as differencing causes one observation to be lost
plot(cbind(as.ts(residuals(reg_y)[-1]), delta_y),
     main = "De-trending Only and With Differencing", ylab = "",
     plot.type = "single",
     col = c("black", "red"), lwd = 2)

legend(x = "top",
       legend = c("After De-trending", "After De-trending and Differencing"),
       col = c("black", "red"), lwd = 2)
```

Perform ADF and KPSS tests on de-trended and differenced variable:

```{r ADF and KPSS tests for delta_y}
delta_y %>% urca::ur.df(type = "drift", selectlags = "AIC") %>% summary()

delta_y %>% urca::ur.kpss(type = "mu", lags = "short") %>% summary()
```

The null hypothesis of the ADF test was rejected (no unit root) and the null hypothesis of the KPSS test cannot be rejected (stationary). In fact, the test statistics would be the same as the test for the differenced RW with positive drift, since the underlying data generating process is simply an addition of a time trend to the RW with positive drift.

## 5 Impact on Forecasting {.tabset .tabset-pills}

In Section 4, methods to transform non-stationary process into stationary process were shown. In this section, I used a slightly more complex model involving ARIMA with deterministic and stochastic trends and see how the methods to achieve stationarity affect forecasting accuracy.

### 5.1 Simulate ARIMA Model

I simulated a simple ARIMA(2, 1, 1) model with a quadratic time trend:

```{r simulate ARIMA with quadratic time trend}
# Number of observations
N = 300

# Model to simulate, need to include coefficients of AR and MA terms
M = list(order = c(2, 1, 1), ar = c(1.02, -0.43), ma = -0.41)

set.seed(456)

stoch_proc <- stats::arima.sim(model = M, n = N, mean = 0.23, sd = 3)

t <- as.ts(0:N)

y <- stoch_proc + 1.39 * t - 0.0027 * t^2

plot(y, main = "Simulated ARIMA(2,1,1) with Quadratic Time Trend", lwd = 2)
```

```{r ADF and KPSS tests for y}
y %>% urca::ur.df(type = "trend", selectlags = "AIC") %>% summary()

y %>% urca::ur.kpss(type = "tau", lags = "short") %>% summary()
```

The null hypothesis for the ADF test cannot be rejected (there is unit root) and the null hypothesis for the KPSS test was rejected (not stationary).

### 5.2 Model Estimation

I attempted to estimate the data as though the true data generating process had not been known. The **`forecast::auto.arima()`** allows level data to be inputted and differencing to be done internally by the function. The only thing then is to decide if a trend is required and if a differencing is needed afterwards.

Firstly, I split the data into training and testing sets.

```{r split training and testing set}
train_y <- as.ts(y[0:281])
train_t <- as.ts(t[0:281])

test_y <- ts(data = y[282:301], start = 282)
test_t <- ts(data = t[282:301], start = 282)
```

Secondly, determine if a fitting a trend is sufficient to obtain residuals with a white noise process.

```{r plot trends}
linear_trend <- lm(train_y ~ train_t)

quad_trend <- lm(train_y ~ train_t + I(train_t^2))

plot(cbind(train_y, fitted(linear_trend), fitted(quad_trend)),
     main = "Actual Training Data and Fitted Trend", ylab = "y",
     plot.type = "single",
     col = c("black", "red", "blue"), lwd = 2)

legend(x = "topleft",
       legend = c("Actual", "Linear trend", "Quadratic trend"),
       col = c("black", "red", "blue"), lwd = 2)
```

We can see that the quadratic trend seem to fit the data better. The residuals from the `quad_trend` model is plotted, along with the residuals of `linear_trend` for comparison:

```{r residual of quad_trend}
plot(as.ts(cbind(residuals(quad_trend), residuals(linear_trend))),
     main = "Residuals from Linear Trend and Quadratic Trend", ylab = "",
     plot.type = "single",
     col = c("black", "red"), lwd = 2)

legend(x = "topleft",
       legend = c("Residuals quad_trend", "Residuals linear_trend"),
       col = c("black", "red"), lwd = 2)
```

Perform ADF and KPSS test on residuals of `quad_trend`:

```{r ADF and KPSS tests for residuals}
residuals(quad_trend) %>% urca::ur.df(type = "drift", selectlags = "AIC") %>% summary()

residuals(quad_trend) %>% urca::ur.kpss(type = "mu", lags = "short") %>% summary()
```

The null hypothesis of the ADF test was rejected (there is no unit root) and the null hypothesis of the KPSS test cannot be rejected (stationary). However, it does not seem like a white noise process, thus differencing may be required after de-trending.

Estimate an ARIMA model using only differencing of the level data:

```{r estimate ARIMA with only differencing}
model1 <- forecast::auto.arima(y = train_y, ic = "aic",
                               seasonal = FALSE, stepwise = FALSE, approximation = FALSE)

model1
```

An ARIMA(3,1,2) model with drift was estimated when the training data was simply differenced.

Estimate an ARIMA model, adding quadratic trend as external regressor:

```{r estimate ARIMA with xreg quad trend}
model2 <- forecast::auto.arima(y = train_y, ic = "aic",
                               xreg = cbind(train_t, train_t^2),
                               seasonal = FALSE, stepwise = FALSE, approximation = FALSE)

model2
```

By adding the time trend, an ARIMA(3,0,2) model was estimated, and no differencing was done since the data was stationary after de-trending as the ADF and KPSS tests have shown earlier.

Estimate an ARIMA model, adding quadratic trend as external regressor and explicitly include differencing:

```{r estimate model2 with differencing included}
model3 <- forecast::auto.arima(y = train_y, ic = "aic", d = 1,
                               xreg = cbind(train_t, train_t^2),
                               seasonal = FALSE, stepwise = FALSE, approximation = FALSE)

model3
```

By including differencing to `model2`, an ARIMA(2,1,3) model was estimated.

### 5.3 Forecast and Evaluation

I forecasted the three models estimated in Section 5.2 for the next 20 periods:

```{r forecast of models}
f_model1 <- forecast::forecast(object = model1, h = 20, level = 95)

f_model2 <- forecast::forecast(object = model2, h = 20, level = 95, xreg = cbind(test_t, test_t^2))

f_model3 <- forecast::forecast(object = model3, h = 20, level = 95, xreg = cbind(test_t, test_t^2))
```

Plot the `f_model1` against the actual value and calculate accuracy measures of forecast:

```{r plot f_model1 against actual value and accuracy}
plot(cbind(y, f_model1$mean, f_model1$lower, f_model1$upper),
     main = "Actual and Forecasted Values of model1", ylab = "y",
     plot.type = "single",
     col = c("black", "red", "gray", "gray"), lwd = 2)

legend(x = "topleft",
       legend = c("Actual", "Forecasted", "Lower and Upper Interval"),
       col = c("black", "red", "gray"), lwd = 2)

data.frame(forecast::accuracy(object = f_model1, x = test_y))
```

Plot the `f_model2` against the actual value and calculate accuracy measures of forecast:

```{r plot f_model2 against actual value and accuracy}
plot(cbind(y, f_model2$mean, f_model2$lower, f_model2$upper),
     main = "Actual and Forecasted Values of model2", ylab = "y",
     plot.type = "single",
     col = c("black", "red", "gray", "gray"), lwd = 2)

legend(x = "topleft",
       legend = c("Actual", "Forecasted", "Lower and Upper Interval"),
       col = c("black", "red", "gray"), lwd = 2)

data.frame(forecast::accuracy(object = f_model2, x = test_y))
```

Plot the `f_model3` against the actual value and calculate accuracy measures of forecast:

```{r plot f_model3 against actual value and accuracy}
plot(cbind(y, f_model3$mean, f_model3$lower, f_model3$upper),
     main = "Actual and Forecasted Values of model3", ylab = "y",
     plot.type = "single",
     col = c("black", "red", "gray", "gray"), lwd = 2)

legend(x = "topleft",
       legend = c("Actual", "Forecasted", "Lower and Upper Interval"),
       col = c("black", "red", "gray"), lwd = 2)

data.frame(forecast::accuracy(object = f_model3, x = test_y))
```

Based on the accuracy measures (from the `Test set` rows), `model3` had better forecasting performance than `model1` and `model2`. Including the trend in model estimation was able to improve forecasts of variables with deterministic trend. However, estimating the trend can be difficult at times. For example, if the fitted quadratic trend predicted an inflection point has occurred even though the variable would continue to be on an upward trend for a period in the future, the forecasts would be impacted and would probably show a down trend. 

## 6 Final Remarks

This project used simulations of random walk models to determine and visualize the difference between deterministic and stochastic trends. An ARIMA model with deterministic and stochastic trend was simulated to see if including the trend into model estimation could improve forecasts. This are simple examples, but real-world data is much more complicated, with changing relationships between variables due to exogenous factors or shocks. In such cases, periodic re-estimation of the model with rolling windows may improve model estimation and forecasts, which I would test in another project.

## References

Iordanova, T. (2022). *An Introduction to Non-Stationary Processes*. Investopedia. Retrieved 3 August 2022, from https://www.investopedia.com/articles/trading/07/stationary.asp#:~:text=Random%20Walk%20with%20Drift%20and,trend%2C%20and%20a%20stochastic%20component

Kotzé, K. *Nonstationarity*. kevinkotze.github.io. Retrieved 3 August 2022, from https://kevinkotze.github.io/ts-6-unit-roots/

*Simulate Random Walk (RW) in R*. Finance Train. Retrieved 1 August 2022, from https://financetrain.com/simulate-random-walk-rw-in-r

Zaiontz, C. *Random Walk*. Real Statistics Using Excel. Retrieved 2 August 2022, from https://www.real-statistics.com/time-series-analysis/stochastic-processes/random-walk/
